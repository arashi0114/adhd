{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0521b638",
   "metadata": {},
   "source": [
    "# Step1. 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3497b33a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m  \u001b[38;5;66;03m# 先导入 pandas\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m..\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mh_elsa_g3.dta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32md:\\AA_hias\\projects\\02ADHD\\adhd\\.venv\\lib\\site-packages\\pandas\\io\\stata.py:2113\u001b[0m, in \u001b[0;36mread_stata\u001b[1;34m(filepath_or_buffer, convert_dates, convert_categoricals, index_col, convert_missing, preserve_dtypes, columns, order_categoricals, chunksize, iterator, compression, storage_options)\u001b[0m\n\u001b[0;32m   2110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reader\n\u001b[0;32m   2112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader:\n\u001b[1;32m-> 2113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\AA_hias\\projects\\02ADHD\\adhd\\.venv\\lib\\site-packages\\pandas\\io\\stata.py:1789\u001b[0m, in \u001b[0;36mStataReader.read\u001b[1;34m(self, nrows, convert_dates, convert_categoricals, index_col, convert_missing, preserve_dtypes, columns, order_categoricals)\u001b[0m\n\u001b[0;32m   1786\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (object_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtyplist[idx]):\n\u001b[0;32m   1787\u001b[0m         data\u001b[38;5;241m.\u001b[39misetitem(idx, data\u001b[38;5;241m.\u001b[39miloc[:, idx]\u001b[38;5;241m.\u001b[39mastype(dtype))\n\u001b[1;32m-> 1789\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_convert_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_missing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_dates:\n\u001b[0;32m   1792\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, fmt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fmtlist):\n",
      "File \u001b[1;32md:\\AA_hias\\projects\\02ADHD\\adhd\\.venv\\lib\\site-packages\\pandas\\io\\stata.py:1867\u001b[0m, in \u001b[0;36mStataReader._do_convert_missing\u001b[1;34m(self, data, convert_missing)\u001b[0m\n\u001b[0;32m   1864\u001b[0m         \u001b[38;5;66;03m# Note: operating on ._values is much faster than directly\u001b[39;00m\n\u001b[0;32m   1865\u001b[0m         \u001b[38;5;66;03m# TODO: can we fix that?\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m         replacement\u001b[38;5;241m.\u001b[39m_values[missing] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m-> 1867\u001b[0m     replacements[i] \u001b[38;5;241m=\u001b[39m replacement\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m replacements:\n\u001b[0;32m   1869\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, value \u001b[38;5;129;01min\u001b[39;00m replacements\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd  # 先导入 pandas\n",
    "\n",
    "df = pd.read_stata(r\"..\\data\\raw\\h_elsa_g3.dta\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663f7f0",
   "metadata": {},
   "source": [
    "# Step2. 筛选变量\n",
    "由于elsa是个很广泛的数据集，我们专注AD早筛的话，很多变量都是不太相关的，可以直接去掉。于是我们采用逐渐缩小范围的方式筛选数据\n",
    "## 1. 筛除不相关变量\n",
    "第一次筛选，留下b,k,l,m,q中的本人相关变量（配偶的就去掉了），以及A中，性别、年龄、婚姻、教育相关的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef5509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "变量种类数： 779\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from temp import count_unique_vars\n",
    "\n",
    "# 获取所有列名列表\n",
    "cols = list(df.columns)\n",
    "\n",
    "#sectionA:demographics_marriage\n",
    "cols_marriage = cols[cols.index(\"r1mstat\"):cols.index(\"r9mstat\")+1]\n",
    "\n",
    "# sectionB:health\n",
    "cols_health = cols[cols.index(\"r1shlt\"):cols.index(\"s9memrys\")+1]\n",
    "cols_health = [col for col in cols_health if not col.startswith('s')]\n",
    "\n",
    "# sectionK:physical measures\n",
    "cols_pm = cols[cols.index(\"r1wspeed1\"):cols.index(\"s6chrothr\")+1]\n",
    "cols_pm = [col for col in cols_pm if not col.startswith('s')]\n",
    "\n",
    "# sectionL: assistance and caregiving   \n",
    "cols_ac = cols[cols.index(\"r6dresshlp\"):cols.index(\"s9gcaresckhpw\")+1]\n",
    "cols_ac = [col for col in cols_ac if not col.startswith('s')]\n",
    "\n",
    "# sectionM:stress\n",
    "cols_stress = cols[cols.index(\"r2satjob\"):cols.index(\"s5dcother\")+1]\n",
    "cols_stress = [col for col in cols_stress if not col.startswith('s')]\n",
    "\n",
    "# sectionQ: psychosocial\n",
    "cols_psycho = cols[cols.index(\"r1depres\"):cols.index(\"s7slfneg2\")+1]\n",
    "cols_psycho = [col for col in cols_psycho if not col.startswith('s')]\n",
    "\n",
    "all_cols1 = [\"ragender\", \"rabyear\", \"raeducl\"] + cols_marriage + cols_health + cols_pm + cols_ac + cols_stress + cols_psycho\n",
    "\n",
    "var_names, var_count = count_unique_vars(all_cols1)\n",
    "print(\"变量种类数：\", var_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d9e32",
   "metadata": {},
   "source": [
    "## 2. 手动删除重复变量\n",
    "针对前面的df_filtered1,手动删除重复信息\n",
    "1. 数据泄露问题，比如是否诊断阿兹海默\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8036408b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "变量种类数： 768\n"
     ]
    }
   ],
   "source": [
    "from temp import remove_multiple_ranges\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(all_cols1, [(\"r1alzhe\",\"radiagdemen\"),(\"r2alzhs\",\"r9memrys\")])\n",
    "\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"变量种类数：\", var_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ad7082",
   "metadata": {},
   "source": [
    "2. 不相关变量去除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca47b8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "变量种类数： 766\n"
     ]
    }
   ],
   "source": [
    "from temp import remove_multiple_ranges\n",
    "\n",
    "#删除 sectionQ——社会阶级地位\n",
    "all_cols2 = remove_multiple_ranges(all_cols2, [(\"r1cantril\",\"r3cantrilc\")])\n",
    "\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"变量种类数：\", var_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ea12b4",
   "metadata": {},
   "source": [
    "2. 单个问题&汇总得分之间的重复。可以抛弃单个，只取汇总得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd15c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adls处理后变量种类数： 690\n",
      "falls处理后变量种类数： 684\n",
      "painfr处理后变量种类数： 683\n",
      "change处理后变量种类数： 660\n",
      "walk处理后变量种类数： 653\n",
      "Blood Pressure and Heart Rate Measurements处理后变量种类数： 639\n",
      "hand grip处理后变量种类数： 629\n",
      "height&weight处理后变量种类数： 619\n",
      "Lung Function Measurements处理后变量种类数： 601\n",
      "Balance Tests处理后变量种类数： 588\n",
      "Leg Raise Tests处理后变量种类数： 584\n",
      "Chair Stand Tests处理后变量种类数： 579\n",
      "Care for ADLs or IADLS: Receives Any Care处理后变量种类数： 576\n",
      "Social Support: Spouse处理后变量种类数： 566\n",
      "Social Support: children处理后变量种类数： 556\n",
      "Social Support: Other Family Members处理后变量种类数： 546\n",
      "Social Support: friends处理后变量种类数： 536\n",
      "Depressive Symptoms: CESD处理后变量种类数： 527\n",
      "Satisfaction with Life Scale处理后变量种类数： 512\n",
      "CASP处理后变量种类数： 488\n"
     ]
    }
   ],
   "source": [
    "from temp import remove_multiple_ranges_with_keep, remove_multiple_ranges\n",
    "\n",
    "\n",
    "# 删除section B中的单个问题,留下RwADLTOT6/RwIADLTOT1M_E/RwNAGI10\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r1walkra\", \"r9nagi8a\")],\n",
    "    [(\"r1adltot6\", \"r9adltot6\"), (\"r1iadltot1m_e\", \"r9iadltot1m_e\"), (\"r1nagi10\", \"r9nagi10\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"adls处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r1fall\", \"r9hip\")],\n",
    "    [(\"r1fallnum\", \"r9fallnum\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"falls处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [(\"r1painfr\", \"r9painfr\")],\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"painfr处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [(\"r2shltc\",\"r9hips\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"change处理后变量种类数：\", var_count)\n",
    "\n",
    "# 删除section K中的单个测量值，留下平均值\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [(\"r1wspeed1\",\"r9wspeed2\"), (\"r1walksft\",\"r9walkothr\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"walk处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [\n",
    "        (\"r2systo1\", \"r8systo3\"),\n",
    "        (\"r2diasto1\", \"r8diasto3\"),\n",
    "        (\"r2pulse1\", \"r8pulse3\"),\n",
    "        (\"r2bpsft\",\"r8bpothr\")\n",
    "    ]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Blood Pressure and Heart Rate Measurements处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [\n",
    "        (\"r2lgrip1\", \"r8lgrip3\"),\n",
    "        (\"r2rgrip1\", \"r8rgrip3\"),\n",
    "        (\"r2gripsft\",\"r8gripothr\")\n",
    "    ]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"hand grip处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [\n",
    "        (\"r2hghtsft\", \"r9wghtothr\"),\n",
    "    ]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"height&weight处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [\n",
    "        (\"r2puff1\", \"r4puff3\"),\n",
    "        (\"r2fvc1\", \"r4fvc3\"),\n",
    "        (\"r2fev1\", \"r4fev3\"),\n",
    "        (\"r2puffsft\",\"r6puffothr_e\")\n",
    "    ]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Lung Function Measurements处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r2sbstan\", \"r6balothr\")],\n",
    "    [(\"r2balance_e\", \"r6balance_e\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Balance Tests处理后变量种类数：\", var_count)\n",
    "\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [(\"r2legrsft\", \"r6legrothr\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Leg Raise Tests处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [(\"r2chrsft\", \"r6chrothr\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Chair Stand Tests处理后变量种类数：\", var_count)\n",
    "\n",
    "# 删除section L——Care for ADLs or IADLS: Receives Any Care中的细分项\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [(\"r6racany\", \"r9rcany\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Care for ADLs or IADLS: Receives Any Care处理后变量种类数：\", var_count)\n",
    "\n",
    "# 删除section M中的单项，留下汇总分\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r1sustdfe\", \"r9ssupportm\")],\n",
    "    [(\"r1ssupport6\",\"r9ssupport6\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Social Support: Spouse处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r1kustdfe\", \"r9ksupportm\")],\n",
    "    [(\"r1ksupport6\",\"r9ksupport6\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Social Support: children处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r1oustdfe\", \"r9osupportm\")],\n",
    "    [(\"r1osupport6\",\"r9osupport6\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Social Support: Other Family Members处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r1fustdfe\", \"r9fsupportm\")],\n",
    "    [(\"r1fsupport6\",\"r9fsupport6\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Social Support: friends处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r1depres\", \"r9cesdm\")],\n",
    "    [(\"r1cesd\",\"r9cesd\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Depressive Symptoms: CESD处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r2lideal\", \"r9satlifez\")],\n",
    "    [(\"r2lsatsc\", \"r9lsatsc\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Satisfaction with Life Scale处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r1ageprv\", \"r9casp12\")],\n",
    "    [\n",
    "        (\"r1cntrlndx6\", \"r9cntrlndx6\"),   # Control (6 items)\n",
    "        (\"r1autondx5\",  \"r9autondx5\"),    # Autonomy (5 items)\n",
    "        (\"r1plsrndx4\",  \"r9plsrndx4\"),   # Pleasure (4 items)\n",
    "        (\"r1slfrlndx4\", \"r9slfrlndx4\"),   # Self-realization (4 items)\n",
    "        (\"r1casp19\",    \"r9casp19\")       # CASP-19 total score\n",
    "    ]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"CASP处理后变量种类数：\", var_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80142b",
   "metadata": {},
   "source": [
    "# 3. 手动筛掉wave太少的变量\n",
    "统计488个变量中，wave-变量的分布情况\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153b55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各波次变量数量统计： {1: 126, 2: 234, 3: 162, 4: 205, 5: 185, 6: 290, 7: 298, 8: 201, 9: 201}\n",
      "各波次缺失变量数量统计： {1: 362, 2: 254, 3: 326, 4: 283, 5: 303, 6: 198, 7: 190, 8: 287, 9: 287}\n",
      "\n",
      "变量矩阵形状: (440, 9)\n",
      "非波次变量数量: 48\n",
      "\n",
      "变量在波次中的分布统计:\n",
      "出现在所有9个波次的变量数: 98\n",
      "出现在8个波次的变量数: 39\n"
     ]
    }
   ],
   "source": [
    "# 重新导入模块以获取新添加的函数\n",
    "import importlib\n",
    "import temp as data_process_utils\n",
    "importlib.reload(data_process_utils)\n",
    "from temp import count_wave_numbers\n",
    "\n",
    "wave_counts = count_wave_numbers(all_cols2)\n",
    "print(\"各波次变量数量统计：\", wave_counts)\n",
    "# 计算各波次缺失变量数量\n",
    "total_vars = 488  # 总变量数\n",
    "missing_counts = {}\n",
    "for wave, count in wave_counts.items():\n",
    "    missing_counts[wave] = total_vars - count\n",
    "print(\"各波次缺失变量数量统计：\", missing_counts)\n",
    "\n",
    "# 将一维变量列表转换为二维DataFrame\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def create_wave_matrix(col_list):\n",
    "    \"\"\"\n",
    "    将变量列表转换为二维矩阵\n",
    "    行：变量名（去掉波次前缀）\n",
    "    列：波次（r1-r9）\n",
    "    值：1表示该波次有该变量，NaN表示没有\n",
    "    注意：\n",
    "    - 保留有波次的变量（r+数字+变量名格式）\n",
    "    - 不符合格式的变量，另外收集起来\n",
    "    \"\"\"\n",
    "    var_names = set()\n",
    "    wave_var_dict = {}\n",
    "    non_wave_vars = []   # 保存不符合格式的变量\n",
    "    \n",
    "    for col in col_list:\n",
    "        match = re.match(r'r(\\d+)([a-zA-Z0-9_]+)', col.lower())\n",
    "        if match:\n",
    "            wave = int(match.group(1))\n",
    "            var_name = match.group(2)\n",
    "            var_names.add(var_name)\n",
    "            if var_name not in wave_var_dict:\n",
    "                wave_var_dict[var_name] = set()\n",
    "            wave_var_dict[var_name].add(wave)\n",
    "        else:\n",
    "            non_wave_vars.append(col)\n",
    "    \n",
    "    waves = list(range(1, 10))  # r1到r9\n",
    "    var_names_sorted = sorted(var_names)\n",
    "    \n",
    "    matrix_data = {}\n",
    "    for wave in waves:\n",
    "        matrix_data[f'r{wave}'] = [\n",
    "            1 if wave in wave_var_dict.get(var, set()) else None\n",
    "            for var in var_names_sorted\n",
    "        ]\n",
    "    \n",
    "    df_matrix = pd.DataFrame(matrix_data, index=var_names_sorted)\n",
    "    return df_matrix, non_wave_vars\n",
    "\n",
    "# 创建波次变量矩阵 & 非波次变量列表\n",
    "wave_matrix, non_wave_vars = create_wave_matrix(all_cols2)\n",
    "print(f\"\\n变量矩阵形状: {wave_matrix.shape}\")\n",
    "print(f\"非波次变量数量: {len(non_wave_vars)}\")\n",
    "\n",
    "# 统计每个变量在多少个波次中出现\n",
    "var_wave_counts = wave_matrix.count(axis=1)\n",
    "print(f\"\\n变量在波次中的分布统计:\")\n",
    "print(f\"出现在所有9个波次的变量数: {sum(var_wave_counts == 9)}\")\n",
    "print(f\"出现在8个波次的变量数: {sum(var_wave_counts == 8)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78095e4",
   "metadata": {},
   "source": [
    "从all_cols2中去掉出现在7个及以下wave里的变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6addcbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "波次过滤统计:\n",
      "- 原始变量总数: 1950\n",
      "- 有波次的变量数: 440\n",
      "- 非波次变量数: 48\n",
      "- 出现在8个或更多波次的变量数:137\n",
      "- 被移除的变量数: 303\n",
      "- 过滤后变量总数: 1242\n",
      "\n",
      "被移除的变量（出现波次<8）:\n",
      "  shltf: 1个波次\n",
      "  shlta: 2个波次\n",
      "  shltaf: 1个波次\n",
      "  rxhrtat: 2个波次\n",
      "  rxosteo: 6个波次\n",
      "  rxdepres: 3个波次\n",
      "  trdepres: 3个波次\n",
      "  trhchol: 3个波次\n",
      "  rxhchol: 7个波次\n",
      "  hipr: 2个波次\n",
      "  ... 还有293个变量被移除\n",
      "\n",
      "过滤后变量种类数: 185\n",
      "\n",
      "过滤完成！all_cols2已更新为只包含出现在8个或更多波次的变量。\n"
     ]
    }
   ],
   "source": [
    "# 从all_cols2中去掉出现在7个及以下wave里的变量，只保留出现在8个或9个wave的变量\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def filter_vars_by_wave_count(col_list, min_waves=8):\n",
    "    \"\"\"\n",
    "    过滤变量列表，只保留出现在min_waves个或更多波次中的变量\n",
    "\n",
    "    Args:\n",
    "        col_list: 变量列表\n",
    "        min_waves: 最少出现的波次数（默认8）\n",
    "\n",
    "    Returns:\n",
    "        filtered_cols: 过滤后的变量列表\n",
    "        removed_cols: 被移除的变量列表\n",
    "    \"\"\"\n",
    "    # 分离有波次的变量和无波次的变量\n",
    "    wave_vars = {}  # {变量名: [出现的波次]}\n",
    "    non_wave_vars = []\n",
    "\n",
    "    for col in col_list:\n",
    "        match = re.match(r'r(\\d+)([a-zA-Z0-9_]+)', col.lower())\n",
    "        if match:\n",
    "            wave = int(match.group(1))\n",
    "            var_name = match.group(2)\n",
    "            if var_name not in wave_vars:\n",
    "                wave_vars[var_name] = []\n",
    "            wave_vars[var_name].append(wave)\n",
    "        else:\n",
    "            non_wave_vars.append(col)\n",
    "\n",
    "    # 统计每个变量出现在多少个波次中\n",
    "    var_wave_counts = {var: len(waves) for var, waves in wave_vars.items()}\n",
    "\n",
    "    # 筛选出现在min_waves个或更多波次的变量\n",
    "    valid_vars = [var for var, count in var_wave_counts.items() if count >= min_waves]\n",
    "    removed_vars = [var for var, count in var_wave_counts.items() if count < min_waves]\n",
    "\n",
    "    # 重建符合条件的变量列表\n",
    "    filtered_cols = []\n",
    "    for var in valid_vars:\n",
    "        for wave in wave_vars[var]:\n",
    "            filtered_cols.append(f\"r{wave}{var}\")\n",
    "\n",
    "    # 添加非波次变量（这些变量保留）\n",
    "    filtered_cols.extend(non_wave_vars)\n",
    "\n",
    "    # 保持原始顺序\n",
    "    filtered_cols = [col for col in col_list if col in filtered_cols]\n",
    "\n",
    "    removed_cols = [col for col in col_list if col not in filtered_cols]\n",
    "\n",
    "    print(f\"波次过滤统计:\")\n",
    "    print(f\"- 原始变量总数: {len(col_list)}\")\n",
    "    print(f\"- 有波次的变量数: {len(wave_vars)}\")\n",
    "    print(f\"- 非波次变量数: {len(non_wave_vars)}\")\n",
    "    print(f\"- 出现在{min_waves}个或更多波次的变量数:{len(valid_vars)}\")\n",
    "    print(f\"- 被移除的变量数: {len(removed_vars)}\")\n",
    "    print(f\"- 过滤后变量总数: {len(filtered_cols)}\")\n",
    "\n",
    "    if removed_vars:\n",
    "        print(f\"\\n被移除的变量（出现波次<{min_waves}）:\")\n",
    "        removed_stats = {var: var_wave_counts[var] for var in removed_vars[:10]}  # 只显示前10个\n",
    "        for var, count in removed_stats.items():\n",
    "            print(f\"  {var}: {count}个波次\")\n",
    "        if len(removed_vars) > 10:\n",
    "            print(f\"  ... 还有{len(removed_vars)-10}个变量被移除\")      \n",
    "\n",
    "    return filtered_cols, removed_cols\n",
    "\n",
    "# 应用过滤器，只保留出现在8个或更多波次的变量\n",
    "all_cols2_filtered, removed_cols = filter_vars_by_wave_count(all_cols2, min_waves=8)\n",
    "\n",
    "# 更新all_cols2\n",
    "all_cols2 = all_cols2_filtered\n",
    "\n",
    "# 重新统计变量种类数\n",
    "from temp import count_unique_vars\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(f\"\\n过滤后变量种类数: {var_count}\")\n",
    "\n",
    "\n",
    "print(f\"\\n过滤完成！all_cols2已更新为只包含出现在8个或更多波次的变量。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1465ca4",
   "metadata": {},
   "source": [
    "## 4. 补充处理\n",
    "对于一些表面缺失、实际上可以修复的变量，采用手动处理的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6750637e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功在位置 14 插入 r3shlta\n",
      "['r9walkre', 'r2systo', 'r4systo', 'r6systo', 'r8systo', 'r2diasto', 'r4diasto', 'r6diasto', 'r8diasto', 'r2pulse', 'r4pulse', 'r6pulse', 'r8pulse', 'r2gripsum', 'r4gripsum', 'r6gripsum', 'r8gripsum', 'r2mheight', 'r4mheight', 'r6mheight']\n"
     ]
    }
   ],
   "source": [
    "# 在 r2shlt 和 r4shlt 之间插入 r3shlta\n",
    "try:\n",
    "    r2shlt_index = all_cols2.index(\"r2shlt\")\n",
    "    # 在 r2shlt 后面插入 r3shlta\n",
    "    all_cols2.insert(r2shlt_index + 1, \"r3shlta\")\n",
    "    print(f\"成功在位置 {r2shlt_index + 1} 插入 r3shlta\")\n",
    "    \n",
    "except ValueError:\n",
    "    print(\"错误: 在 all_cols2 中未找到 r2shlt\")\n",
    "except NameError:\n",
    "    print(\"错误: all_cols2 未定义，请先运行前面的数据处理步骤\")\n",
    "\n",
    "# 插入sectionK中的变量： 血压 心率 握力 身高体重 BMI\n",
    "to_insert = [\"SYSTO\", \"DIASTO\", \"PULSE\", \"GRIPSUM\", \"MHEIGHT\", \"MWEIGHT\", \"MBMI\"]\n",
    "waves = [2, 4, 6, 8]\n",
    "\n",
    "# 按每个变量依次扩展波次并变小写\n",
    "expanded_vars = [f\"r{w}{v}\".lower() for v in to_insert for w in waves]\n",
    "\n",
    "# 找到 r9walkre 的索引\n",
    "idx = all_cols2.index(\"r9walkre\")\n",
    "\n",
    "# 插入到 r9walkre 后面\n",
    "all_cols2 = all_cols2[:idx+1] + expanded_vars + all_cols2[idx+1:]\n",
    "\n",
    "# 查看插入后的部分\n",
    "print(all_cols2[idx:idx+20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7436bba6",
   "metadata": {},
   "source": [
    "# Step3. 计算出发病的lable\n",
    "根据挑选出的认知变量计算总分，如果在3个变量里，有缺失＞30%，则该样本的这一栏分数为空"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d90ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_cognition = cols[cols.index(\"r1tr20\"):cols.index(\"r9tr20\")+1] + cols[cols.index(\"r1orient\"):cols.index(\"r9orient\")+1] + cols[cols.index(\"r1verbf\"): cols.index(\"r9verbf\")+1]\n",
    "cols_diagnosed = cols[cols.index(\"r1memrye\"):cols.index(\"r9memrye\")+1]\n",
    "df_cognition = df[cols_cognition + cols_diagnosed]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "col_idx = df_cognition.columns.get_loc('r5verbf')\n",
    "if isinstance(col_idx, int):\n",
    "    df_cognition.insert(col_idx + 1, 'r6verbf', np.nan)\n",
    "\n",
    "\n",
    "# 1. 计算认知分数baseline（baseline取wave1）\n",
    "def compute_baseline_stats(df_baseline):\n",
    "    results = []\n",
    "\n",
    "    # 去掉前缀 r1\n",
    "    domain_names = [col[2:] if col.startswith('r1') else col for col in df_baseline.columns]\n",
    "\n",
    "    # 计算每个领域均值和标准差\n",
    "    for col, domain in zip(df_baseline.columns, domain_names):\n",
    "        mean_val = df_baseline[col].mean()\n",
    "        std_val = df_baseline[col].std()\n",
    "        results.append({'domain': domain, 'mean': mean_val, 'std': std_val})\n",
    "\n",
    "    # 计算每个领域 z 分数\n",
    "    z_df = ((df_baseline - df_baseline.mean()) / df_baseline.std()).mean(axis=1)\n",
    "\n",
    "    # 计算全局分数均值和标准差\n",
    "    global_mean = z_df.mean()\n",
    "    global_std = z_df.std()\n",
    "    results.append({'domain': 'global', 'mean': global_mean, 'std': global_std})\n",
    "\n",
    "    # 构建结果 DataFrame\n",
    "    result_df = pd.DataFrame(results).set_index('domain')\n",
    "\n",
    "    return result_df\n",
    "baseline = compute_baseline_stats(df_cognition[[\"r1tr20\", \"r1orient\",\"r1verbf\"]])\n",
    "\n",
    "# 2. 针对每个wave计算发病lable\n",
    "# （是否患病：racogimp_label cont:-1（从不患病）/1-9（开始患病wave）/NaN（由于信息缺失无效）; \n",
    "# 当前wave距离患病wave的时间：r[wave]cogimpt_label cont：-1（无效，比如从未发病，或者当前wave是onset之后，yearcont为负数）/year cont（≥0）/NaN（（由于conimp缺失无效）））\n",
    "\n",
    "def compute_cogimp_labels(df_compute, baseline_stats, thred=-1.5, wave_interval=2):\n",
    "    \"\"\"\n",
    "    计算各 wave 的认知障碍标签和发病信息。\n",
    "    \n",
    "    参数：\n",
    "        df_compute: DataFrame，包含各 wave 认知领域和诊断列\n",
    "        baseline_stats: DataFrame，index=领域名, columns=['mean','std']，wave1 baseline\n",
    "        thred: float，全局 z 阈值，低于视为认知障碍\n",
    "        wave_interval: int，每两个 wave 间隔年数\n",
    "    \n",
    "    返回：\n",
    "        result_df: DataFrame，每行受访者\n",
    "            - racogimp_label: 1–9 onset wave, -1 从不发病, NaN 信息缺失\n",
    "            - r[wave]cogimpt_label: 当前 wave 到 onset wave 的年份（>=0），无效设 -1\n",
    "    \"\"\"\n",
    "    result_df = pd.DataFrame(index=df_compute.index)\n",
    "    global_z = pd.DataFrame(index=df_compute.index)\n",
    "    \n",
    "    # 遍历 wave1–wave8，计算出9个wave的全局 z\n",
    "    for wave in range(1, 9):\n",
    "        # 找出该 wave 的领域列\n",
    "        memory_var = f'r{wave}tr20'\n",
    "        orientation_var = f'r{wave}orient'\n",
    "        executive_var = f'r{wave}verbf'\n",
    "\n",
    "        all_vars = {'tr20': memory_var, 'orient': orientation_var, 'verbf': executive_var}\n",
    "        \n",
    "        # 计算每个领域的 z 分数\n",
    "        domain_z = pd.DataFrame(index=df_compute.index)\n",
    "        for domain, var in all_vars.items():\n",
    "            if var:\n",
    "                mean_val = baseline_stats.loc[domain, 'mean']\n",
    "                std_val = baseline_stats.loc[domain, 'std']\n",
    "                domain_z[domain] = (df_compute[var] - mean_val) / std_val\n",
    "        \n",
    "        # 全局 z\n",
    "        global_z[wave] = domain_z.sum(axis=1, min_count=2)/domain_z.count(axis=1)\n",
    "        global_z[wave] = (global_z[wave] - baseline_stats.loc[\"global\", 'mean'])/baseline_stats.loc[\"global\", 'std']\n",
    "\n",
    "    # 生成 racogimp_label\n",
    "    def compute_racogimp_label(global_z, df_compute, threshold= thred, missing_ratio=0.3):\n",
    "        def label_row(row):\n",
    "            # 1. 缺失值占比\n",
    "            if row.isna().mean() > missing_ratio:\n",
    "                return np.nan\n",
    "            # 2. 找第一个满足条件的 wave\n",
    "            for wave in row.index:\n",
    "                diag_val = df_compute.loc[row.name, f'r{wave}memrye']\n",
    "                if row[wave] < threshold or diag_val == \"1.yes\":\n",
    "                    return int(wave)  # 返回 wave 编号\n",
    "            # 3. 找不到则返回 -1\n",
    "            return -1\n",
    "        \n",
    "        return global_z.apply(label_row, axis=1)\n",
    "    result_df['racogimp_label'] = compute_racogimp_label(global_z, df_compute)\n",
    "\n",
    "    \n",
    "    # 生成 r[wave]cogimpt_label\n",
    "    for wave in range(1, 9):\n",
    "        onset_wave = result_df['racogimp_label']\n",
    "        result_df[f'r{wave}cogimpt_label'] = np.where(\n",
    "            onset_wave.isna(),\n",
    "            np.nan,  # 如果原来就是缺失就保持 NaN\n",
    "            np.where(\n",
    "                onset_wave == -1,\n",
    "                -1,  # 如果是 -1 就直接保留 -1\n",
    "                np.where(\n",
    "                    (onset_wave - wave) * wave_interval < 0,  # 如果算出来是负数\n",
    "                    -1,  # 统一改成 -1\n",
    "                    (onset_wave - wave) * wave_interval  # 否则用原本的值\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    \n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    "cognitive_results = compute_cogimp_labels(df_cognition,baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef25140",
   "metadata": {},
   "source": [
    "# Step4. 筛选样本\n",
    "1. 构造具有所有待研究变量的df（19802，186+2=188）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98716907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "变量处理完后的变量个数： 194\n"
     ]
    }
   ],
   "source": [
    "df_filtered_final = pd.concat([df[all_cols2], cognitive_results], axis=1)\n",
    "\n",
    "# 修改手动添加的r3shlta的名字\n",
    "df_filtered_final = df_filtered_final.rename(columns={\"r3shlta\": \"r3shlt\"})\n",
    "\n",
    "# 统计变量个数\n",
    "from temp import count_unique_vars\n",
    "var_names, var_count = count_unique_vars(list(df_filtered_final))\n",
    "print(\"变量处理完后的变量个数：\", var_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e018c",
   "metadata": {},
   "source": [
    "2. 筛选样本：① cognitive_impairment_label不为NaN（有lable）；②稀疏度＜30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b279fb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 稀疏度（缺失比例）统计 ===\n",
      "最小稀疏度: 0.063\n",
      "最大稀疏度: 0.984\n",
      "平均稀疏度: 0.602\n",
      "\n",
      "=== 稀疏度分布（分桶）===\n",
      "(0.7, 0.9]       4539\n",
      "(0.9, 1.0]       4295\n",
      "(-0.001, 0.3]    3877\n",
      "(0.3, 0.5]       3701\n",
      "(0.5, 0.7]       3390\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 计算每行稀疏度（缺失比例）\n",
    "row_sparsity = df_filtered_final.isna().mean(axis=1)\n",
    "\n",
    "# 打印基本统计\n",
    "print(\"=== 稀疏度（缺失比例）统计 ===\")\n",
    "print(f\"最小稀疏度: {row_sparsity.min():.3f}\")\n",
    "print(f\"最大稀疏度: {row_sparsity.max():.3f}\")\n",
    "print(f\"平均稀疏度: {row_sparsity.mean():.3f}\")\n",
    "\n",
    "# 看看分布（例如按区间统计）\n",
    "print(\"\\n=== 稀疏度分布（分桶）===\")\n",
    "print(row_sparsity.value_counts(bins=[0,0.3,0.5,0.7,0.9,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tcrzy6s07jo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "条件1 - racogimp_label不为NaN的样本数: 6165\n",
      "条件2 - 稀疏度＜30%的样本数: 3870\n",
      "条件3 - 不在wave1发病的样本数: 19690\n",
      "同时满足三个条件的样本数: 3802\n",
      "\n",
      "筛选后的数据集形状: (3802, 1280)\n",
      "筛选后的样本数: 3802\n",
      "\n",
      "筛选后数据集的基本统计:\n",
      "- global_cognitive_z_score的描述统计:\n",
      "count    3802.000000\n",
      "mean       -0.169648\n",
      "std         2.329115\n",
      "min        -1.000000\n",
      "25%        -1.000000\n",
      "50%        -1.000000\n",
      "75%        -1.000000\n",
      "max         8.000000\n",
      "Name: racogimp_label, dtype: float64\n",
      "- 整体稀疏度统计:\n",
      "  最小稀疏度: 0.063\n",
      "  最大稀疏度: 0.299\n",
      "  平均稀疏度: 0.197\n"
     ]
    }
   ],
   "source": [
    "# 首先筛选 global_cognitive_z_score 不为 NaN 的样本\n",
    "condition1 = df_filtered_final['racogimp_label'].notna()\n",
    "print(f\"条件1 - racogimp_label不为NaN的样本数: {condition1.sum()}\")\n",
    "\n",
    "# 条件2：稀疏度 ＜ 30%\n",
    "condition2 = df_filtered_final.isna().mean(axis=1) < 0.3\n",
    "print(f\"条件2 - 稀疏度＜30%的样本数: {condition2.sum()}\")\n",
    "\n",
    "# 条件3：不在wave1发病\n",
    "condition3 = df_filtered_final['racogimp_label'] != 1\n",
    "print(f\"条件3 - 不在wave1发病的样本数: {condition3.sum()}\")\n",
    "\n",
    "# 同时满足三个条件的样本\n",
    "all_conditions = condition1 & condition2 & condition3\n",
    "print(f\"同时满足三个条件的样本数: {all_conditions.sum()}\")\n",
    "\n",
    "# 应用筛选\n",
    "df_filtered_final = df_filtered_final[all_conditions].copy()\n",
    "\n",
    "print(f\"\\n筛选后的数据集形状: {df_filtered_final.shape}\")\n",
    "print(f\"筛选后的样本数: {df_filtered_final.shape[0]}\")\n",
    "\n",
    "# 显示筛选后数据的基本信息\n",
    "print(f\"\\n筛选后数据集的基本统计:\")\n",
    "print(f\"- global_cognitive_z_score的描述统计:\")\n",
    "print(df_filtered_final['racogimp_label'].describe())\n",
    "print(f\"- 整体稀疏度统计:\")\n",
    "final_sparsity = df_filtered_final.isna().mean(axis=1)\n",
    "print(f\"  最小稀疏度: {final_sparsity.min():.3f}\")\n",
    "print(f\"  最大稀疏度: {final_sparsity.max():.3f}\")\n",
    "print(f\"  平均稀疏度: {final_sparsity.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c556bf2",
   "metadata": {},
   "source": [
    "# Step5. 一次性确定的补充特征\n",
    "非随机变量的中值插补"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4osc2udjqdn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础变量\n",
    "vars_base = [\"systo\", \"diasto\", \"pulse\", \"gripsum\", \"mheight\", \"mweight\", \"mbmi\"]\n",
    "\n",
    "df_filled = df_filtered_final.copy()\n",
    "\n",
    "for v in vars_base:\n",
    "    # --- wave1 = wave2 ---\n",
    "    colname = f\"r1{v}\"\n",
    "    refcol = f\"r2{v}\"\n",
    "    df_filled.insert(df_filled.columns.get_loc(refcol), colname, df_filled[refcol])\n",
    "\n",
    "    # --- wave3 = (wave2, wave4) 中位数 ---\n",
    "    colname = f\"r3{v}\"\n",
    "    refcol = f\"r4{v}\"\n",
    "    df_filled.insert(df_filled.columns.get_loc(refcol), colname, \n",
    "                     df_filled[[f\"r2{v}\", f\"r4{v}\"]].median(axis=1, skipna=True))\n",
    "\n",
    "    # --- wave5 = (wave4, wave6) 中位数 ---\n",
    "    colname = f\"r5{v}\"\n",
    "    refcol = f\"r6{v}\"\n",
    "    df_filled.insert(df_filled.columns.get_loc(refcol), colname, \n",
    "                     df_filled[[f\"r4{v}\", f\"r6{v}\"]].median(axis=1, skipna=True))\n",
    "\n",
    "    # --- wave7 = (wave6, wave8) 中位数 ---\n",
    "    colname = f\"r7{v}\"\n",
    "    refcol = f\"r8{v}\"\n",
    "    df_filled.insert(df_filled.columns.get_loc(refcol), colname, \n",
    "                     df_filled[[f\"r6{v}\", f\"r8{v}\"]].median(axis=1, skipna=True))\n",
    "\n",
    "    # --- wave9 = wave8 ---\n",
    "    colname = f\"r9{v}\"\n",
    "    refcol = f\"r8{v}\"  # 插在r8后面\n",
    "    df_filled.insert(df_filled.columns.get_loc(refcol) + 1, colname, df_filled[refcol])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc675868",
   "metadata": {},
   "source": [
    "# Step6.保存cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c3e56cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 保存目录\n",
    "save_dir = r\"D:\\AA_hias\\projects\\02ADHD\\adhd\\data\\cleaned\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 自动生成文件名\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"df_cleaned_{timestamp}.csv\"\n",
    "save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "# 保存文件\n",
    "df_filled.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7971c2f3",
   "metadata": {},
   "source": [
    "2. 整个数据集缺失值的多重插补"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f61acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AA_hias\\projects\\02ADHD\\adhd\\.venv\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存: D:\\AA_hias\\projects\\02ADHD\\adhd\\data\\processed\\df_filtered_final_imputed_1_20250901_012512.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AA_hias\\projects\\02ADHD\\adhd\\.venv\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存: D:\\AA_hias\\projects\\02ADHD\\adhd\\data\\processed\\df_filtered_final_imputed_2_20250901_015120.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AA_hias\\projects\\02ADHD\\adhd\\.venv\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存: D:\\AA_hias\\projects\\02ADHD\\adhd\\data\\processed\\df_filtered_final_imputed_3_20250901_021211.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AA_hias\\projects\\02ADHD\\adhd\\.venv\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存: D:\\AA_hias\\projects\\02ADHD\\adhd\\data\\processed\\df_filtered_final_imputed_4_20250901_023709.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AA_hias\\projects\\02ADHD\\adhd\\.venv\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存: D:\\AA_hias\\projects\\02ADHD\\adhd\\data\\processed\\df_filtered_final_imputed_5_20250901_030243.csv\n",
      "共生成并保存 5 个插补后的数据集\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 你的原始数据\n",
    "df_final = df_filled.copy()\n",
    "\n",
    "# 插补次数\n",
    "m = 5  \n",
    "\n",
    "# 保存目录\n",
    "save_dir = r\"D:\\AA_hias\\projects\\02ADHD\\adhd\\data\\processed\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 拆分数值列 & 非数值列\n",
    "num_cols = df_final.select_dtypes(include=[\"number\"]).columns\n",
    "cat_cols = df_final.select_dtypes(exclude=[\"number\"]).columns\n",
    "\n",
    "df_num = df_final[num_cols]\n",
    "df_cat = df_final[cat_cols]\n",
    "\n",
    "# 分类变量：用众数填充\n",
    "imputer_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "df_cat_imputed = pd.DataFrame(\n",
    "    imputer_cat.fit_transform(df_cat),\n",
    "    columns=cat_cols\n",
    ")\n",
    "\n",
    "# 保存每个插补后的数据集\n",
    "imputed_datasets = []\n",
    "\n",
    "for i in range(m):\n",
    "    # 数值变量多重插补·\n",
    "    imputer_num = IterativeImputer(random_state=i)\n",
    "    df_num_imputed = pd.DataFrame(\n",
    "        imputer_num.fit_transform(df_num),\n",
    "        columns=num_cols\n",
    "    )\n",
    "\n",
    "    # 合并：保持原始列顺序\n",
    "    df_imputed = pd.concat([df_num_imputed, df_cat_imputed], axis=1)[df_final.columns]\n",
    "\n",
    "    imputed_datasets.append(df_imputed)\n",
    "\n",
    "    # 自动生成文件名\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"df_filtered_final_imputed_{i+1}_{timestamp}.csv\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "    # 保存文件\n",
    "    df_imputed.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"已保存: {save_path}\")\n",
    "\n",
    "print(f\"共生成并保存 {m} 个插补后的数据集\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0911ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from datetime import datetime\n",
    "\n",
    "# # 指定保存目录\n",
    "# save_dir = r\"D:\\AA_hias\\projects\\02ADHD\\adhd\\data\\processed\"  \n",
    "# os.makedirs(save_dir, exist_ok=True)  # 如果目录不存在，就自动创建\n",
    "\n",
    "# # 自动生成文件名\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# filename = f\"df_filtered_final_imputed_{timestamp}.csv\"\n",
    "\n",
    "# # 拼接完整路径\n",
    "# save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "# # 保存\n",
    "# df_filtered_final_imputed.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# print(f\"文件已保存到: {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
