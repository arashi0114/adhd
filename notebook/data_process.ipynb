{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0521b638",
   "metadata": {},
   "source": [
    "# Step1. 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3497b33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idauniq idauniqc  pn pnc  hh1hhid  hh2hhid  hh3hhid  hh4hhid  hh5hhid  \\\n",
      "0   100001   100001   4   4  14458.0  11870.0  15553.0  17241.0  11052.0   \n",
      "1   100005   100005   2   2  13012.0      NaN      NaN      NaN  13327.0   \n",
      "2   100006   100006   3   3  12415.0  12608.0  13496.0  14156.0  14253.0   \n",
      "3   100007   100007   4   4  15591.0  12585.0  14930.0  16165.0  16347.0   \n",
      "4   100009   100009   3   3  15798.0  15014.0  13922.0  11245.0  10024.0   \n",
      "\n",
      "   hh6hhid  ...  s6slfpos2a  s7slfpos2a  r7slfpos2 s7slfpos2 r6slfneg2a  \\\n",
      "0      NaN  ...         NaN         NaN        NaN       NaN        NaN   \n",
      "1      NaN  ...         NaN         NaN        NaN       NaN        NaN   \n",
      "2  10656.0  ...         NaN         NaN        NaN       NaN        NaN   \n",
      "3  15127.0  ...         6.0         NaN        NaN       NaN        NaN   \n",
      "4  11311.0  ...         4.0         3.0        NaN       NaN        NaN   \n",
      "\n",
      "  r7slfneg2a s6slfneg2a s7slfneg2a r7slfneg2 s7slfneg2  \n",
      "0        NaN        NaN        NaN       NaN       NaN  \n",
      "1        NaN        NaN        NaN       NaN       NaN  \n",
      "2        NaN        NaN        NaN       NaN       NaN  \n",
      "3        NaN        1.0        NaN       NaN       NaN  \n",
      "4        NaN        2.0        1.0       NaN       NaN  \n",
      "\n",
      "[5 rows x 11779 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # 先导入 pandas\n",
    "\n",
    "df = pd.read_stata(r\"..\\data\\raw\\h_elsa_g3.dta\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663f7f0",
   "metadata": {},
   "source": [
    "# Step2. 筛选变量\n",
    "由于elsa是个很广泛的数据集，我们专注AD早筛的话，很多变量都是不太相关的，可以直接去掉。于是我们采用逐渐缩小范围的方式筛选数据\n",
    "## 1. 筛除不相关变量\n",
    "第一次筛选，留下b,k,l,m,q中的本人相关变量（配偶的就去掉了），以及A中，性别、年龄、婚姻、教育相关的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2eef5509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "变量种类数： 779\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from temp import count_unique_vars\n",
    "\n",
    "# 获取所有列名列表\n",
    "cols = list(df.columns)\n",
    "\n",
    "#sectionA:demographics_marriage\n",
    "cols_marriage = cols[cols.index(\"r1mstat\"):cols.index(\"r9mstat\")+1]\n",
    "\n",
    "# sectionB:health\n",
    "cols_health = cols[cols.index(\"r1shlt\"):cols.index(\"s9memrys\")+1]\n",
    "cols_health = [col for col in cols_health if not col.startswith('s')]\n",
    "\n",
    "# sectionK:physical measures\n",
    "cols_pm = cols[cols.index(\"r1wspeed1\"):cols.index(\"s6chrothr\")+1]\n",
    "cols_pm = [col for col in cols_pm if not col.startswith('s')]\n",
    "\n",
    "# sectionL: assistance and caregiving   \n",
    "cols_ac = cols[cols.index(\"r6dresshlp\"):cols.index(\"s9gcaresckhpw\")+1]\n",
    "cols_ac = [col for col in cols_ac if not col.startswith('s')]\n",
    "\n",
    "# sectionM:stress\n",
    "cols_stress = cols[cols.index(\"r2satjob\"):cols.index(\"s5dcother\")+1]\n",
    "cols_stress = [col for col in cols_stress if not col.startswith('s')]\n",
    "\n",
    "# sectionQ: psychosocial\n",
    "cols_psycho = cols[cols.index(\"r1depres\"):cols.index(\"s7slfneg2\")+1]\n",
    "cols_psycho = [col for col in cols_psycho if not col.startswith('s')]\n",
    "\n",
    "all_cols1 = [\"ragender\", \"rabyear\", \"raeducl\"] + cols_marriage + cols_health + cols_pm + cols_ac + cols_stress + cols_psycho\n",
    "\n",
    "var_names, var_count = count_unique_vars(all_cols1)\n",
    "print(\"变量种类数：\", var_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d9e32",
   "metadata": {},
   "source": [
    "## 2. 手动删除重复变量\n",
    "针对前面的df_filtered1,手动删除重复信息\n",
    "1. 数据泄露问题，比如是否诊断阿兹海默\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8036408b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "变量种类数： 768\n"
     ]
    }
   ],
   "source": [
    "from temp import remove_multiple_ranges\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(all_cols1, [(\"r1alzhe\",\"radiagdemen\"),(\"r2alzhs\",\"r9memrys\")])\n",
    "\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"变量种类数：\", var_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ad7082",
   "metadata": {},
   "source": [
    "2. 不相关变量去除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca47b8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "变量种类数： 766\n"
     ]
    }
   ],
   "source": [
    "from temp import remove_multiple_ranges\n",
    "\n",
    "#删除 sectionQ——社会阶级地位\n",
    "all_cols2 = remove_multiple_ranges(all_cols2, [(\"r1cantril\",\"r3cantrilc\")])\n",
    "\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"变量种类数：\", var_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ea12b4",
   "metadata": {},
   "source": [
    "2. 单个问题&汇总得分之间的重复。可以抛弃单个，只取汇总得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0bdd15c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adls处理后变量种类数： 690\n",
      "falls处理后变量种类数： 684\n",
      "painfr处理后变量种类数： 683\n",
      "change处理后变量种类数： 660\n",
      "walk处理后变量种类数： 653\n",
      "Blood Pressure and Heart Rate Measurements处理后变量种类数： 639\n",
      "hand grip处理后变量种类数： 629\n",
      "height&weight处理后变量种类数： 619\n",
      "Lung Function Measurements处理后变量种类数： 601\n",
      "Balance Tests处理后变量种类数： 588\n",
      "Leg Raise Tests处理后变量种类数： 584\n",
      "Chair Stand Tests处理后变量种类数： 579\n",
      "Care for ADLs or IADLS: Receives Any Care处理后变量种类数： 576\n",
      "Social Support: Spouse处理后变量种类数： 566\n",
      "Social Support: children处理后变量种类数： 556\n",
      "Social Support: Other Family Members处理后变量种类数： 546\n",
      "Social Support: friends处理后变量种类数： 536\n",
      "Depressive Symptoms: CESD处理后变量种类数： 527\n",
      "Satisfaction with Life Scale处理后变量种类数： 512\n",
      "CASP处理后变量种类数： 488\n"
     ]
    }
   ],
   "source": [
    "from temp import remove_multiple_ranges_with_keep, remove_multiple_ranges\n",
    "\n",
    "\n",
    "# 删除section B中的单个问题,留下RwADLTOT6/RwIADLTOT1M_E/RwNAGI10\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r1walkra\", \"r9nagi8a\")],\n",
    "    [(\"r1adltot6\", \"r9adltot6\"), (\"r1iadltot1m_e\", \"r9iadltot1m_e\"), (\"r1nagi10\", \"r9nagi10\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"adls处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r1fall\", \"r9hip\")],\n",
    "    [(\"r1fallnum\", \"r9fallnum\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"falls处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [(\"r1painfr\", \"r9painfr\")],\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"painfr处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [(\"r2shltc\",\"r9hips\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"change处理后变量种类数：\", var_count)\n",
    "\n",
    "# 删除section K中的单个测量值，留下平均值\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [(\"r1wspeed1\",\"r9wspeed2\"), (\"r1walksft\",\"r9walkothr\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"walk处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [\n",
    "        (\"r2systo1\", \"r8systo3\"),\n",
    "        (\"r2diasto1\", \"r8diasto3\"),\n",
    "        (\"r2pulse1\", \"r8pulse3\"),\n",
    "        (\"r2bpsft\",\"r8bpothr\")\n",
    "    ]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Blood Pressure and Heart Rate Measurements处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [\n",
    "        (\"r2lgrip1\", \"r8lgrip3\"),\n",
    "        (\"r2rgrip1\", \"r8rgrip3\"),\n",
    "        (\"r2gripsft\",\"r8gripothr\")\n",
    "    ]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"hand grip处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [\n",
    "        (\"r2hghtsft\", \"r9wghtothr\"),\n",
    "    ]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"height&weight处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [\n",
    "        (\"r2puff1\", \"r4puff3\"),\n",
    "        (\"r2fvc1\", \"r4fvc3\"),\n",
    "        (\"r2fev1\", \"r4fev3\"),\n",
    "        (\"r2puffsft\",\"r6puffothr_e\")\n",
    "    ]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Lung Function Measurements处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r2sbstan\", \"r6balothr\")],\n",
    "    [(\"r2balance_e\", \"r6balance_e\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Balance Tests处理后变量种类数：\", var_count)\n",
    "\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [(\"r2legrsft\", \"r6legrothr\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Leg Raise Tests处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [(\"r2chrsft\", \"r6chrothr\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Chair Stand Tests处理后变量种类数：\", var_count)\n",
    "\n",
    "# 删除section L——Care for ADLs or IADLS: Receives Any Care中的细分项\n",
    "all_cols2 = remove_multiple_ranges(\n",
    "    all_cols2,\n",
    "    [(\"r6racany\", \"r9rcany\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Care for ADLs or IADLS: Receives Any Care处理后变量种类数：\", var_count)\n",
    "\n",
    "# 删除section M中的单项，留下汇总分\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r1sustdfe\", \"r9ssupportm\")],\n",
    "    [(\"r1ssupport6\",\"r9ssupport6\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Social Support: Spouse处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r1kustdfe\", \"r9ksupportm\")],\n",
    "    [(\"r1ksupport6\",\"r9ksupport6\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Social Support: children处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r1oustdfe\", \"r9osupportm\")],\n",
    "    [(\"r1osupport6\",\"r9osupport6\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Social Support: Other Family Members处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r1fustdfe\", \"r9fsupportm\")],\n",
    "    [(\"r1fsupport6\",\"r9fsupport6\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Social Support: friends处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r1depres\", \"r9cesdm\")],\n",
    "    [(\"r1cesd\",\"r9cesd\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Depressive Symptoms: CESD处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r2lideal\", \"r9satlifez\")],\n",
    "    [(\"r2lsatsc\", \"r9lsatsc\")]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"Satisfaction with Life Scale处理后变量种类数：\", var_count)\n",
    "\n",
    "all_cols2 = remove_multiple_ranges_with_keep(\n",
    "    all_cols2,\n",
    "    [(\"r1ageprv\", \"r9casp12\")],\n",
    "    [\n",
    "        (\"r1cntrlndx6\", \"r9cntrlndx6\"),   # Control (6 items)\n",
    "        (\"r1autondx5\",  \"r9autondx5\"),    # Autonomy (5 items)\n",
    "        (\"r1plsrndx4\",  \"r9plsrndx4\"),   # Pleasure (4 items)\n",
    "        (\"r1slfrlndx4\", \"r9slfrlndx4\"),   # Self-realization (4 items)\n",
    "        (\"r1casp19\",    \"r9casp19\")       # CASP-19 total score\n",
    "    ]\n",
    ")\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(\"CASP处理后变量种类数：\", var_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80142b",
   "metadata": {},
   "source": [
    "# 3. 手动筛掉wave太少的变量\n",
    "统计488个变量中，wave-变量的分布情况\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b153b55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各波次变量数量统计： {1: 126, 2: 234, 3: 162, 4: 205, 5: 185, 6: 290, 7: 298, 8: 201, 9: 201}\n",
      "各波次缺失变量数量统计： {1: 362, 2: 254, 3: 326, 4: 283, 5: 303, 6: 198, 7: 190, 8: 287, 9: 287}\n",
      "\n",
      "变量矩阵形状: (440, 9)\n",
      "非波次变量数量: 48\n",
      "\n",
      "变量在波次中的分布统计:\n",
      "出现在所有9个波次的变量数: 98\n",
      "出现在8个波次的变量数: 39\n"
     ]
    }
   ],
   "source": [
    "# 重新导入模块以获取新添加的函数\n",
    "import importlib\n",
    "import temp as data_process_utils\n",
    "importlib.reload(data_process_utils)\n",
    "from temp import count_wave_numbers\n",
    "\n",
    "wave_counts = count_wave_numbers(all_cols2)\n",
    "print(\"各波次变量数量统计：\", wave_counts)\n",
    "# 计算各波次缺失变量数量\n",
    "total_vars = 488  # 总变量数\n",
    "missing_counts = {}\n",
    "for wave, count in wave_counts.items():\n",
    "    missing_counts[wave] = total_vars - count\n",
    "print(\"各波次缺失变量数量统计：\", missing_counts)\n",
    "\n",
    "# 将一维变量列表转换为二维DataFrame\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def create_wave_matrix(col_list):\n",
    "    \"\"\"\n",
    "    将变量列表转换为二维矩阵\n",
    "    行：变量名（去掉波次前缀）\n",
    "    列：波次（r1-r9）\n",
    "    值：1表示该波次有该变量，NaN表示没有\n",
    "    注意：\n",
    "    - 保留有波次的变量（r+数字+变量名格式）\n",
    "    - 不符合格式的变量，另外收集起来\n",
    "    \"\"\"\n",
    "    var_names = set()\n",
    "    wave_var_dict = {}\n",
    "    non_wave_vars = []   # 保存不符合格式的变量\n",
    "    \n",
    "    for col in col_list:\n",
    "        match = re.match(r'r(\\d+)([a-zA-Z0-9_]+)', col.lower())\n",
    "        if match:\n",
    "            wave = int(match.group(1))\n",
    "            var_name = match.group(2)\n",
    "            var_names.add(var_name)\n",
    "            if var_name not in wave_var_dict:\n",
    "                wave_var_dict[var_name] = set()\n",
    "            wave_var_dict[var_name].add(wave)\n",
    "        else:\n",
    "            non_wave_vars.append(col)\n",
    "    \n",
    "    waves = list(range(1, 10))  # r1到r9\n",
    "    var_names_sorted = sorted(var_names)\n",
    "    \n",
    "    matrix_data = {}\n",
    "    for wave in waves:\n",
    "        matrix_data[f'r{wave}'] = [\n",
    "            1 if wave in wave_var_dict.get(var, set()) else None\n",
    "            for var in var_names_sorted\n",
    "        ]\n",
    "    \n",
    "    df_matrix = pd.DataFrame(matrix_data, index=var_names_sorted)\n",
    "    return df_matrix, non_wave_vars\n",
    "\n",
    "# 创建波次变量矩阵 & 非波次变量列表\n",
    "wave_matrix, non_wave_vars = create_wave_matrix(all_cols2)\n",
    "print(f\"\\n变量矩阵形状: {wave_matrix.shape}\")\n",
    "print(f\"非波次变量数量: {len(non_wave_vars)}\")\n",
    "\n",
    "# 统计每个变量在多少个波次中出现\n",
    "var_wave_counts = wave_matrix.count(axis=1)\n",
    "print(f\"\\n变量在波次中的分布统计:\")\n",
    "print(f\"出现在所有9个波次的变量数: {sum(var_wave_counts == 9)}\")\n",
    "print(f\"出现在8个波次的变量数: {sum(var_wave_counts == 8)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78095e4",
   "metadata": {},
   "source": [
    "从all_cols2中去掉出现在7个及以下wave里的变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6addcbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "波次过滤统计:\n",
      "- 原始变量总数: 1950\n",
      "- 有波次的变量数: 440\n",
      "- 非波次变量数: 48\n",
      "- 出现在8个或更多波次的变量数:137\n",
      "- 被移除的变量数: 303\n",
      "- 过滤后变量总数: 1242\n",
      "\n",
      "被移除的变量（出现波次<8）:\n",
      "  shltf: 1个波次\n",
      "  shlta: 2个波次\n",
      "  shltaf: 1个波次\n",
      "  rxhrtat: 2个波次\n",
      "  rxosteo: 6个波次\n",
      "  rxdepres: 3个波次\n",
      "  trdepres: 3个波次\n",
      "  trhchol: 3个波次\n",
      "  rxhchol: 7个波次\n",
      "  hipr: 2个波次\n",
      "  ... 还有293个变量被移除\n",
      "\n",
      "过滤后变量种类数: 185\n",
      "\n",
      "过滤完成！all_cols2已更新为只包含出现在8个或更多波次的变量。\n"
     ]
    }
   ],
   "source": [
    "# 从all_cols2中去掉出现在7个及以下wave里的变量，只保留出现在8个或9个wave的变量\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def filter_vars_by_wave_count(col_list, min_waves=8):\n",
    "    \"\"\"\n",
    "    过滤变量列表，只保留出现在min_waves个或更多波次中的变量\n",
    "\n",
    "    Args:\n",
    "        col_list: 变量列表\n",
    "        min_waves: 最少出现的波次数（默认8）\n",
    "\n",
    "    Returns:\n",
    "        filtered_cols: 过滤后的变量列表\n",
    "        removed_cols: 被移除的变量列表\n",
    "    \"\"\"\n",
    "    # 分离有波次的变量和无波次的变量\n",
    "    wave_vars = {}  # {变量名: [出现的波次]}\n",
    "    non_wave_vars = []\n",
    "\n",
    "    for col in col_list:\n",
    "        match = re.match(r'r(\\d+)([a-zA-Z0-9_]+)', col.lower())\n",
    "        if match:\n",
    "            wave = int(match.group(1))\n",
    "            var_name = match.group(2)\n",
    "            if var_name not in wave_vars:\n",
    "                wave_vars[var_name] = []\n",
    "            wave_vars[var_name].append(wave)\n",
    "        else:\n",
    "            non_wave_vars.append(col)\n",
    "\n",
    "    # 统计每个变量出现在多少个波次中\n",
    "    var_wave_counts = {var: len(waves) for var, waves in wave_vars.items()}\n",
    "\n",
    "    # 筛选出现在min_waves个或更多波次的变量\n",
    "    valid_vars = [var for var, count in var_wave_counts.items() if count >= min_waves]\n",
    "    removed_vars = [var for var, count in var_wave_counts.items() if count < min_waves]\n",
    "\n",
    "    # 重建符合条件的变量列表\n",
    "    filtered_cols = []\n",
    "    for var in valid_vars:\n",
    "        for wave in wave_vars[var]:\n",
    "            filtered_cols.append(f\"r{wave}{var}\")\n",
    "\n",
    "    # 添加非波次变量（这些变量保留）\n",
    "    filtered_cols.extend(non_wave_vars)\n",
    "\n",
    "    # 保持原始顺序\n",
    "    filtered_cols = [col for col in col_list if col in filtered_cols]\n",
    "\n",
    "    removed_cols = [col for col in col_list if col not in filtered_cols]\n",
    "\n",
    "    print(f\"波次过滤统计:\")\n",
    "    print(f\"- 原始变量总数: {len(col_list)}\")\n",
    "    print(f\"- 有波次的变量数: {len(wave_vars)}\")\n",
    "    print(f\"- 非波次变量数: {len(non_wave_vars)}\")\n",
    "    print(f\"- 出现在{min_waves}个或更多波次的变量数:{len(valid_vars)}\")\n",
    "    print(f\"- 被移除的变量数: {len(removed_vars)}\")\n",
    "    print(f\"- 过滤后变量总数: {len(filtered_cols)}\")\n",
    "\n",
    "    if removed_vars:\n",
    "        print(f\"\\n被移除的变量（出现波次<{min_waves}）:\")\n",
    "        removed_stats = {var: var_wave_counts[var] for var in removed_vars[:10]}  # 只显示前10个\n",
    "        for var, count in removed_stats.items():\n",
    "            print(f\"  {var}: {count}个波次\")\n",
    "        if len(removed_vars) > 10:\n",
    "            print(f\"  ... 还有{len(removed_vars)-10}个变量被移除\")      \n",
    "\n",
    "    return filtered_cols, removed_cols\n",
    "\n",
    "# 应用过滤器，只保留出现在8个或更多波次的变量\n",
    "all_cols2_filtered, removed_cols = filter_vars_by_wave_count(all_cols2, min_waves=8)\n",
    "\n",
    "# 更新all_cols2\n",
    "all_cols2 = all_cols2_filtered\n",
    "\n",
    "# 重新统计变量种类数\n",
    "from temp import count_unique_vars\n",
    "var_names, var_count = count_unique_vars(all_cols2)\n",
    "print(f\"\\n过滤后变量种类数: {var_count}\")\n",
    "\n",
    "\n",
    "print(f\"\\n过滤完成！all_cols2已更新为只包含出现在8个或更多波次的变量。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1465ca4",
   "metadata": {},
   "source": [
    "## 4. 补充处理\n",
    "对于一些表面缺失、实际上可以修复的变量，采用手动处理的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6750637e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功在位置 14 插入 r3shlta\n",
      "['r9walkre', 'r2systo', 'r4systo', 'r6systo', 'r8systo', 'r2diasto', 'r4diasto', 'r6diasto', 'r8diasto', 'r2pulse', 'r4pulse', 'r6pulse', 'r8pulse', 'r2gripsum', 'r4gripsum', 'r6gripsum', 'r8gripsum', 'r2mheight', 'r4mheight', 'r6mheight']\n"
     ]
    }
   ],
   "source": [
    "# 在 r2shlt 和 r4shlt 之间插入 r3shlta\n",
    "try:\n",
    "    r2shlt_index = all_cols2.index(\"r2shlt\")\n",
    "    # 在 r2shlt 后面插入 r3shlta\n",
    "    all_cols2.insert(r2shlt_index + 1, \"r3shlta\")\n",
    "    print(f\"成功在位置 {r2shlt_index + 1} 插入 r3shlta\")\n",
    "    \n",
    "except ValueError:\n",
    "    print(\"错误: 在 all_cols2 中未找到 r2shlt\")\n",
    "except NameError:\n",
    "    print(\"错误: all_cols2 未定义，请先运行前面的数据处理步骤\")\n",
    "\n",
    "# 插入sectionK中的变量： 血压 心率 握力 身高体重 BMI\n",
    "to_insert = [\"SYSTO\", \"DIASTO\", \"PULSE\", \"GRIPSUM\", \"MHEIGHT\", \"MWEIGHT\", \"MBMI\"]\n",
    "waves = [2, 4, 6, 8]\n",
    "\n",
    "# 按每个变量依次扩展波次并变小写\n",
    "expanded_vars = [f\"r{w}{v}\".lower() for v in to_insert for w in waves]\n",
    "\n",
    "# 找到 r9walkre 的索引\n",
    "idx = all_cols2.index(\"r9walkre\")\n",
    "\n",
    "# 插入到 r9walkre 后面\n",
    "all_cols2 = all_cols2[:idx+1] + expanded_vars + all_cols2[idx+1:]\n",
    "\n",
    "# 查看插入后的部分\n",
    "print(all_cols2[idx:idx+20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7436bba6",
   "metadata": {},
   "source": [
    "# Step3. 计算出发病的lable\n",
    "根据挑选出的认知变量计算总分，如果在3个变量里，有缺失＞30%，则该样本的这一栏分数为空"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d90ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "识别到认知变量:\n",
      "- 记忆: 9 个变量\n",
      "- 定向: 9 个变量\n",
      "- 执行功能: 8 个变量\n",
      "\n",
      "计算结果:\n",
      "- 总样本数: 19802\n",
      "- 有有效认知数据的样本数: 18866\n",
      "- 曾经发病的样本数: 3310\n",
      "- 从未发病的样本数: 15556\n",
      "- 无有效数据的样本数: 936\n",
      "\n",
      "发病wave分布:\n",
      "- Wave 1: 794 人\n",
      "- Wave 2: 436 人\n",
      "- Wave 3: 390 人\n",
      "- Wave 4: 450 人\n",
      "- Wave 5: 342 人\n",
      "- Wave 7: 331 人\n",
      "- Wave 8: 273 人\n",
      "- Wave 9: 294 人\n",
      "\n",
      "结果预览:\n",
      "   cognitive_impairment_label  onset_wave  r1cognitive_impairment_time  \\\n",
      "0                        <NA>        <NA>                          NaN   \n",
      "1                          -1        <NA>                          NaN   \n",
      "2                          -1        <NA>                          NaN   \n",
      "3                          -1        <NA>                          NaN   \n",
      "4                          -1        <NA>                          NaN   \n",
      "5                        <NA>        <NA>                          NaN   \n",
      "6                          -1        <NA>                          NaN   \n",
      "7                           1           3                          4.0   \n",
      "8                          -1        <NA>                          NaN   \n",
      "9                        <NA>        <NA>                          NaN   \n",
      "\n",
      "   r2cognitive_impairment_time  r3cognitive_impairment_time  \\\n",
      "0                          NaN                          NaN   \n",
      "1                          NaN                          NaN   \n",
      "2                          NaN                          NaN   \n",
      "3                          NaN                          NaN   \n",
      "4                          NaN                          NaN   \n",
      "5                          NaN                          NaN   \n",
      "6                          NaN                          NaN   \n",
      "7                          2.0                          0.0   \n",
      "8                          NaN                          NaN   \n",
      "9                          NaN                          NaN   \n",
      "\n",
      "   r4cognitive_impairment_time  r5cognitive_impairment_time  \\\n",
      "0                          NaN                          NaN   \n",
      "1                          NaN                          NaN   \n",
      "2                          NaN                          NaN   \n",
      "3                          NaN                          NaN   \n",
      "4                          NaN                          NaN   \n",
      "5                          NaN                          NaN   \n",
      "6                          NaN                          NaN   \n",
      "7                         -2.0                         -4.0   \n",
      "8                          NaN                          NaN   \n",
      "9                          NaN                          NaN   \n",
      "\n",
      "   r6cognitive_impairment_time  r7cognitive_impairment_time  \\\n",
      "0                          NaN                          NaN   \n",
      "1                          NaN                          NaN   \n",
      "2                          NaN                          NaN   \n",
      "3                          NaN                          NaN   \n",
      "4                          NaN                          NaN   \n",
      "5                          NaN                          NaN   \n",
      "6                          NaN                          NaN   \n",
      "7                         -6.0                         -8.0   \n",
      "8                          NaN                          NaN   \n",
      "9                          NaN                          NaN   \n",
      "\n",
      "   r8cognitive_impairment_time  r9cognitive_impairment_time  \\\n",
      "0                          NaN                          NaN   \n",
      "1                          NaN                          NaN   \n",
      "2                          NaN                          NaN   \n",
      "3                          NaN                          NaN   \n",
      "4                          NaN                          NaN   \n",
      "5                          NaN                          NaN   \n",
      "6                          NaN                          NaN   \n",
      "7                        -10.0                        -12.0   \n",
      "8                          NaN                          NaN   \n",
      "9                          NaN                          NaN   \n",
      "\n",
      "   global_cognitive_z_score  \n",
      "0                       NaN  \n",
      "1                 -0.901943  \n",
      "2                       NaN  \n",
      "3                  0.420989  \n",
      "4                       NaN  \n",
      "5                       NaN  \n",
      "6                  0.676549  \n",
      "7                       NaN  \n",
      "8                       NaN  \n",
      "9                       NaN  \n",
      "\n",
      "标签分布:\n",
      "cognitive_impairment_label\n",
      "-1    15556\n",
      "1      3310\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "时间变量示例 (r1cognitive_impairment_time, r5cognitive_impairment_time):\n",
      "   cognitive_impairment_label  onset_wave  r1cognitive_impairment_time  \\\n",
      "0                        <NA>        <NA>                          NaN   \n",
      "1                          -1        <NA>                          NaN   \n",
      "2                          -1        <NA>                          NaN   \n",
      "3                          -1        <NA>                          NaN   \n",
      "4                          -1        <NA>                          NaN   \n",
      "5                        <NA>        <NA>                          NaN   \n",
      "6                          -1        <NA>                          NaN   \n",
      "7                           1           3                          4.0   \n",
      "8                          -1        <NA>                          NaN   \n",
      "9                        <NA>        <NA>                          NaN   \n",
      "\n",
      "   r5cognitive_impairment_time  \n",
      "0                          NaN  \n",
      "1                          NaN  \n",
      "2                          NaN  \n",
      "3                          NaN  \n",
      "4                          NaN  \n",
      "5                          NaN  \n",
      "6                          NaN  \n",
      "7                         -4.0  \n",
      "8                          NaN  \n",
      "9                          NaN  \n"
     ]
    }
   ],
   "source": [
    "cols_cognition = cols[cols.index(\"r1tr20\"):cols.index(\"r9tr20\")+1] + cols[cols.index(\"r1orient\"):cols.index(\"r9orient\")+1] + cols[cols.index(\"r1verbf\"): cols.index(\"r9verbf\")+1]\n",
    "cols_diagnosed = cols[cols.index(\"r1memrye\"):cols.index(\"r9memrye\")+1]\n",
    "df_cognition = df[cols_cognition + cols_diagnosed]\n",
    "\n",
    "\n",
    "# 1. 计算认知分数baseline（baseline取wave1）\n",
    "def compute_baseline_stats(df_baseline):\n",
    "    results = []\n",
    "\n",
    "    # 去掉前缀 r1\n",
    "    domain_names = [col[2:] if col.startswith('r1') else col for col in df_baseline.columns]\n",
    "\n",
    "    # 计算每个领域均值和标准差\n",
    "    for col, domain in zip(df_baseline.columns, domain_names):\n",
    "        mean_val = df_baseline[col].mean()\n",
    "        std_val = df_baseline[col].std()\n",
    "        results.append({'domain': domain, 'mean': mean_val, 'std': std_val})\n",
    "\n",
    "    # 计算每个领域 z 分数\n",
    "    z_df = (df_baseline - df_baseline.mean()) / df_baseline.std()\n",
    "    df_baseline['global'] = z_df.mean(axis=1)\n",
    "\n",
    "    # 计算全局分数均值和标准差\n",
    "    global_mean = df_baseline['global'].mean()\n",
    "    global_std = df_baseline['global'].std()\n",
    "    results.append({'domain': 'global', 'mean': global_mean, 'std': global_std})\n",
    "\n",
    "    # 构建结果 DataFrame\n",
    "    result_df = pd.DataFrame(results).set_index('domain')\n",
    "\n",
    "    return result_df\n",
    "baseline = compute_baseline_stats(df_cognition[[\"r1tr20\", \"r1orient\",\"r1verbf\"]])\n",
    "\n",
    "# 2. 针对每个wave计算发病lable\n",
    "# （是否患病：racogimp_label cont:-1（从不患病）/1-9（开始患病wave）/NaN（由于信息缺失无效）; \n",
    "# 当前wave距离患病wave的时间：r[wave]cogimpt cont：-1（无效，比如从未发病，或者当前wave是onset之后，yearcont为负数）/year cont（≥0）/NaN（（由于conimp缺失无效）））\n",
    "\n",
    "# 2.1 根据是否被诊断出认知问题判断\n",
    "\n",
    "# 2.2 计算认知分数并和baseline比较判断\n",
    "import numpy as np\n",
    "\n",
    "def compute_cogimp_labels(df_compute, baseline_stats, thred=-1.5, wave_interval=2):\n",
    "    \"\"\"\n",
    "    计算各 wave 的认知障碍标签和发病信息。\n",
    "    \n",
    "    参数：\n",
    "        df_compute: DataFrame，包含各 wave 认知领域和诊断列\n",
    "        baseline_stats: DataFrame，index=领域名, columns=['mean','std']，wave1 baseline\n",
    "        thred: float，全局 z 阈值，低于视为认知障碍\n",
    "        wave_interval: int，每两个 wave 间隔年数\n",
    "    \n",
    "    返回：\n",
    "        result_df: DataFrame，每行受访者\n",
    "            - racogimp_label: 1–9 onset wave, -1 从不发病, NaN 信息缺失\n",
    "            - r[wave]cogimpt: 当前 wave 到 onset wave 的年份（>=0），无效设 -1\n",
    "    \"\"\"\n",
    "    result_df = pd.DataFrame(index=df_compute.index)\n",
    "    global_z = pd.DataFrame(index=df_compute.index)\n",
    "    \n",
    "    # 遍历 wave1–wave8，计算出9个wave的全局 z\n",
    "    for wave in range(1, 9):\n",
    "        # 找出该 wave 的领域列\n",
    "        memory_var = f'r{wave}tr20'\n",
    "        orientation_var = f'r{wave}orient'\n",
    "        executive_var = f'r{wave}verbf'\n",
    "\n",
    "        all_vars = {'tr20': memory_var, 'orient': orientation_var, 'verbf': executive_var}\n",
    "        \n",
    "        # 计算每个领域的 z 分数\n",
    "        domain_z = pd.DataFrame(index=df_compute.index)\n",
    "        for domain, var in all_vars.items():\n",
    "            if var:\n",
    "                mean_val = baseline_stats.loc[domain, 'mean']\n",
    "                std_val = baseline_stats.loc[domain, 'std']\n",
    "                domain_z[domain] = (df_compute[var] - mean_val) / std_val\n",
    "        \n",
    "        # 全局 z\n",
    "        global_z[wave] = domain_z.mean(axis=1, skipna=True, min_count=2)\n",
    "        global_z[wave] = (global_z[wave] - baseline_stats.loc[\"global\", 'mean'])/baseline_stats.loc[\"global\", 'std']\n",
    "\n",
    "    # 生成 racogimp_label\n",
    "    def compute_racogimp_label(global_z, df_compute, threshold=1.5, missing_ratio=0.3):\n",
    "        def label_row(row):\n",
    "            # 1. 缺失值占比\n",
    "            if row.isna().mean() > missing_ratio:\n",
    "                return np.nan\n",
    "            # 2. 找第一个满足条件的 wave\n",
    "            for wave in row.index:\n",
    "                diag_val = df_compute.loc[row.name, f'r{wave}memrye']\n",
    "                if row[wave] < threshold or diag_val == \"1.yes\":\n",
    "                    return int(wave)  # 返回 wave 编号\n",
    "            # 3. 找不到则返回 -1\n",
    "            return -1\n",
    "        \n",
    "        return global_z.apply(label_row, axis=1)\n",
    "    racogimp_label = compute_racogimp_label(global_z, df_compute)\n",
    "\n",
    "    \n",
    "    # 生成 r[wave]cogimpt\n",
    "    for wave in range(2, 9):\n",
    "        yearcont = (onset_wave - wave) * wave_interval\n",
    "        # 无效：从未发病(-1) 或当前 wave 在 onset 之后 (yearcont<0)\n",
    "        yearcont_mask = (racogimp_label==-1) | (yearcont<0)\n",
    "        yearcont[yearcont_mask] = -1\n",
    "        result_df[f'r{wave}cogimpt'] = yearcont\n",
    "    \n",
    "    result_df['racogimp_label'] = racogimp_label\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    "cognitive_results = compute_cogimp_labels(df_cognition,baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef25140",
   "metadata": {},
   "source": [
    "# Step4. 筛选样本\n",
    "1. 构造具有所有待研究变量的df（19802，186+2=188）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98716907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "变量处理完后的变量个数： 196\n",
      "\n",
      "认知障碍标签分布:\n",
      "cognitive_impairment_label\n",
      "-1    15556\n",
      "1      3310\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "时间变量示例:\n",
      "   cognitive_impairment_label  onset_wave  r1cognitive_impairment_time  \\\n",
      "0                        <NA>        <NA>                          NaN   \n",
      "1                          -1        <NA>                          NaN   \n",
      "2                          -1        <NA>                          NaN   \n",
      "3                          -1        <NA>                          NaN   \n",
      "4                          -1        <NA>                          NaN   \n",
      "5                        <NA>        <NA>                          NaN   \n",
      "6                          -1        <NA>                          NaN   \n",
      "7                           1           3                          4.0   \n",
      "8                          -1        <NA>                          NaN   \n",
      "9                        <NA>        <NA>                          NaN   \n",
      "\n",
      "   r3cognitive_impairment_time  r5cognitive_impairment_time  \n",
      "0                          NaN                          NaN  \n",
      "1                          NaN                          NaN  \n",
      "2                          NaN                          NaN  \n",
      "3                          NaN                          NaN  \n",
      "4                          NaN                          NaN  \n",
      "5                          NaN                          NaN  \n",
      "6                          NaN                          NaN  \n",
      "7                          0.0                         -4.0  \n",
      "8                          NaN                          NaN  \n",
      "9                          NaN                          NaN  \n",
      "\n",
      "发病案例的时间变量验证（发病在wave3的情况）:\n",
      "发病wave: 3\n",
      "时间变量值:\n",
      "  r1cognitive_impairment_time: 4.0 (期望: 4)\n",
      "  r2cognitive_impairment_time: 2.0 (期望: 2)\n",
      "  r3cognitive_impairment_time: 0.0 (期望: 0)\n",
      "  r4cognitive_impairment_time: -2.0 (期望: -2)\n",
      "  r5cognitive_impairment_time: -4.0 (期望: -4)\n"
     ]
    }
   ],
   "source": [
    "df_filtered_final = pd.concat([df[all_cols2], cognitive_results], axis=1)\n",
    "\n",
    "# 修改手动添加的r3shlta的名字\n",
    "df_filtered_final = df_filtered_final.rename(columns={\"r3shlta\": \"r3shlt\"})\n",
    "\n",
    "# 统计变量个数\n",
    "from temp import count_unique_vars\n",
    "var_names, var_count = count_unique_vars(list(df_filtered_final))\n",
    "print(\"变量处理完后的变量个数：\", var_count)\n",
    "\n",
    "# 显示认知障碍标签的分布\n",
    "print(f\"\\n认知障碍标签分布:\")\n",
    "print(cognitive_results['cognitive_impairment_label'].value_counts().sort_index())\n",
    "\n",
    "# 显示时间变量的一些例子\n",
    "print(f\"\\n时间变量示例:\")\n",
    "sample_cols = ['cognitive_impairment_label', 'onset_wave', 'r1cognitive_impairment_time', 'r3cognitive_impairment_time', 'r5cognitive_impairment_time']\n",
    "available_cols = [col for col in sample_cols if col in cognitive_results.columns]\n",
    "print(cognitive_results[available_cols].head(10))\n",
    "\n",
    "# 对于发病案例，显示时间变量的计算逻辑验证\n",
    "print(f\"\\n发病案例的时间变量验证（发病在wave3的情况）:\")\n",
    "onset_wave3_mask = (cognitive_results['onset_wave'] == 3) & (cognitive_results['cognitive_impairment_label'] == 1)\n",
    "if onset_wave3_mask.any():\n",
    "    sample_case = cognitive_results[onset_wave3_mask].head(1)\n",
    "    time_cols = [col for col in cognitive_results.columns if 'cognitive_impairment_time' in col]\n",
    "    print(\"发病wave:\", sample_case['onset_wave'].values[0])\n",
    "    print(\"时间变量值:\")\n",
    "    for col in time_cols[:5]:  # 只显示前5个\n",
    "        wave_num = col[1]  # 提取wave数字\n",
    "        expected_time = (3 - int(wave_num)) * 2  # 期望的时间值\n",
    "        actual_time = sample_case[col].values[0]\n",
    "        print(f\"  {col}: {actual_time} (期望: {expected_time})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e018c",
   "metadata": {},
   "source": [
    "2. 筛选样本：① cognitive_impairment_label不为NaN（有lable）；②稀疏度＜30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b279fb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 稀疏度（缺失比例）统计 ===\n",
      "最小稀疏度: 0.071\n",
      "最大稀疏度: 0.984\n",
      "平均稀疏度: 0.603\n",
      "\n",
      "=== 稀疏度分布（分桶）===\n",
      "(0.7, 0.9]       4531\n",
      "(0.9, 1.0]       4280\n",
      "(-0.001, 0.3]    3811\n",
      "(0.3, 0.5]       3759\n",
      "(0.5, 0.7]       3421\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 计算每行稀疏度（缺失比例）\n",
    "row_sparsity = df_filtered_final.isna().mean(axis=1)\n",
    "\n",
    "# 打印基本统计\n",
    "print(\"=== 稀疏度（缺失比例）统计 ===\")\n",
    "print(f\"最小稀疏度: {row_sparsity.min():.3f}\")\n",
    "print(f\"最大稀疏度: {row_sparsity.max():.3f}\")\n",
    "print(f\"平均稀疏度: {row_sparsity.mean():.3f}\")\n",
    "\n",
    "# 看看分布（例如按区间统计）\n",
    "print(\"\\n=== 稀疏度分布（分桶）===\")\n",
    "print(row_sparsity.value_counts(bins=[0,0.3,0.5,0.7,0.9,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tcrzy6s07jo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "条件1 - global_cognitive_z_score不为NaN的样本数: 8059\n",
      "条件2 - 稀疏度＜30%的样本数: 3811\n",
      "同时满足两个条件的样本数: 3287\n",
      "\n",
      "筛选后的数据集形状: (3287, 1283)\n",
      "筛选后的样本数: 3287\n",
      "\n",
      "筛选后数据集的基本统计:\n",
      "- global_cognitive_z_score的描述统计:\n",
      "count    3287.000000\n",
      "mean       -0.186756\n",
      "std         1.090888\n",
      "min        -5.859625\n",
      "25%        -0.654722\n",
      "50%        -0.012898\n",
      "75%         0.498222\n",
      "max         2.382079\n",
      "Name: global_cognitive_z_score, dtype: float64\n",
      "- 整体稀疏度统计:\n",
      "  最小稀疏度: 0.071\n",
      "  最大稀疏度: 0.299\n",
      "  平均稀疏度: 0.194\n"
     ]
    }
   ],
   "source": [
    "# 首先筛选 global_cognitive_z_score 不为 NaN 的样本\n",
    "condition1 = df_filtered_final['cognitive_impairment_label'].notna()\n",
    "print(f\"条件1 - cognitive_impairment_label不为NaN的样本数: {condition1.sum()}\")\n",
    "\n",
    "# 计算每行的非缺失值比例（即非稀疏度）\n",
    "# 计算稀疏度：缺失值比例 = 1 - 非缺失值比例\n",
    "sparsity = df_filtered_final.isna().mean(axis=1)\n",
    "non_sparsity = 1 - sparsity\n",
    "\n",
    "# 条件2：稀疏度 ＜ 30%\n",
    "condition2 = sparsity < 0.3\n",
    "print(f\"条件2 - 稀疏度＜30%的样本数: {condition2.sum()}\")\n",
    "\n",
    "# 同时满足两个条件的样本\n",
    "both_conditions = condition1 & condition2\n",
    "print(f\"同时满足两个条件的样本数: {both_conditions.sum()}\")\n",
    "\n",
    "# 应用筛选\n",
    "df_filtered_final = df_filtered_final[both_conditions].copy()\n",
    "\n",
    "print(f\"\\n筛选后的数据集形状: {df_filtered_final.shape}\")\n",
    "print(f\"筛选后的样本数: {df_filtered_final.shape[0]}\")\n",
    "\n",
    "# 显示筛选后数据的基本信息\n",
    "print(f\"\\n筛选后数据集的基本统计:\")\n",
    "print(f\"- global_cognitive_z_score的描述统计:\")\n",
    "print(df_filtered_final['global_cognitive_z_score'].describe())\n",
    "print(f\"- 整体稀疏度统计:\")\n",
    "final_sparsity = df_filtered_final.isna().mean(axis=1)\n",
    "print(f\"  最小稀疏度: {final_sparsity.min():.3f}\")\n",
    "print(f\"  最大稀疏度: {final_sparsity.max():.3f}\")\n",
    "print(f\"  平均稀疏度: {final_sparsity.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c556bf2",
   "metadata": {},
   "source": [
    "# Step5. 缺失值处理\n",
    "1. 非随机变量的中值插补"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4osc2udjqdn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础变量\n",
    "vars_base = [\"systo\", \"diasto\", \"pulse\", \"gripsum\", \"mheight\", \"mweight\", \"mbmi\"]\n",
    "\n",
    "df_filled = df_filtered_final.copy()\n",
    "\n",
    "for v in vars_base:\n",
    "    # --- wave1 = wave2 ---\n",
    "    colname = f\"r1{v}\"\n",
    "    refcol = f\"r2{v}\"\n",
    "    df_filled.insert(df_filled.columns.get_loc(refcol), colname, df_filled[refcol])\n",
    "\n",
    "    # --- wave3 = (wave2, wave4) 中位数 ---\n",
    "    colname = f\"r3{v}\"\n",
    "    refcol = f\"r4{v}\"\n",
    "    df_filled.insert(df_filled.columns.get_loc(refcol), colname, \n",
    "                     df_filled[[f\"r2{v}\", f\"r4{v}\"]].median(axis=1, skipna=True))\n",
    "\n",
    "    # --- wave5 = (wave4, wave6) 中位数 ---\n",
    "    colname = f\"r5{v}\"\n",
    "    refcol = f\"r6{v}\"\n",
    "    df_filled.insert(df_filled.columns.get_loc(refcol), colname, \n",
    "                     df_filled[[f\"r4{v}\", f\"r6{v}\"]].median(axis=1, skipna=True))\n",
    "\n",
    "    # --- wave7 = (wave6, wave8) 中位数 ---\n",
    "    colname = f\"r7{v}\"\n",
    "    refcol = f\"r8{v}\"\n",
    "    df_filled.insert(df_filled.columns.get_loc(refcol), colname, \n",
    "                     df_filled[[f\"r6{v}\", f\"r8{v}\"]].median(axis=1, skipna=True))\n",
    "\n",
    "    # --- wave9 = wave8 ---\n",
    "    colname = f\"r9{v}\"\n",
    "    refcol = f\"r8{v}\"  # 插在r8后面\n",
    "    df_filled.insert(df_filled.columns.get_loc(refcol) + 1, colname, df_filled[refcol])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7971c2f3",
   "metadata": {},
   "source": [
    "2. 整个数据集缺失值的多重插补"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f61acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AA_hias\\projects\\02ADHD\\adhd\\.venv\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存: D:\\AA_hias\\projects\\02ADHD\\adhd\\data\\processed\\df_filtered_final_imputed_1_20250901_012512.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AA_hias\\projects\\02ADHD\\adhd\\.venv\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存: D:\\AA_hias\\projects\\02ADHD\\adhd\\data\\processed\\df_filtered_final_imputed_2_20250901_015120.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AA_hias\\projects\\02ADHD\\adhd\\.venv\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存: D:\\AA_hias\\projects\\02ADHD\\adhd\\data\\processed\\df_filtered_final_imputed_3_20250901_021211.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AA_hias\\projects\\02ADHD\\adhd\\.venv\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存: D:\\AA_hias\\projects\\02ADHD\\adhd\\data\\processed\\df_filtered_final_imputed_4_20250901_023709.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AA_hias\\projects\\02ADHD\\adhd\\.venv\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存: D:\\AA_hias\\projects\\02ADHD\\adhd\\data\\processed\\df_filtered_final_imputed_5_20250901_030243.csv\n",
      "共生成并保存 5 个插补后的数据集\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 你的原始数据\n",
    "df_final = df_filled.copy()\n",
    "\n",
    "# 插补次数\n",
    "m = 5  \n",
    "\n",
    "# 保存目录\n",
    "save_dir = r\"D:\\AA_hias\\projects\\02ADHD\\adhd\\data\\processed\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 拆分数值列 & 非数值列\n",
    "num_cols = df_final.select_dtypes(include=[\"number\"]).columns\n",
    "cat_cols = df_final.select_dtypes(exclude=[\"number\"]).columns\n",
    "\n",
    "df_num = df_final[num_cols]\n",
    "df_cat = df_final[cat_cols]\n",
    "\n",
    "# 分类变量：用众数填充\n",
    "imputer_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "df_cat_imputed = pd.DataFrame(\n",
    "    imputer_cat.fit_transform(df_cat),\n",
    "    columns=cat_cols\n",
    ")\n",
    "\n",
    "# 保存每个插补后的数据集\n",
    "imputed_datasets = []\n",
    "\n",
    "for i in range(m):\n",
    "    # 数值变量多重插补·\n",
    "    imputer_num = IterativeImputer(random_state=i)\n",
    "    df_num_imputed = pd.DataFrame(\n",
    "        imputer_num.fit_transform(df_num),\n",
    "        columns=num_cols\n",
    "    )\n",
    "\n",
    "    # 合并：保持原始列顺序\n",
    "    df_imputed = pd.concat([df_num_imputed, df_cat_imputed], axis=1)[df_final.columns]\n",
    "\n",
    "    imputed_datasets.append(df_imputed)\n",
    "\n",
    "    # 自动生成文件名\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"df_filtered_final_imputed_{i+1}_{timestamp}.csv\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "    # 保存文件\n",
    "    df_imputed.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"已保存: {save_path}\")\n",
    "\n",
    "print(f\"共生成并保存 {m} 个插补后的数据集\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0911ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from datetime import datetime\n",
    "\n",
    "# # 指定保存目录\n",
    "# save_dir = r\"D:\\AA_hias\\projects\\02ADHD\\adhd\\data\\processed\"  \n",
    "# os.makedirs(save_dir, exist_ok=True)  # 如果目录不存在，就自动创建\n",
    "\n",
    "# # 自动生成文件名\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# filename = f\"df_filtered_final_imputed_{timestamp}.csv\"\n",
    "\n",
    "# # 拼接完整路径\n",
    "# save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "# # 保存\n",
    "# df_filtered_final_imputed.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# print(f\"文件已保存到: {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
