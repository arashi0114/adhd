#!/usr/bin/env python3
"""
ADHDè®¤çŸ¥éšœç¢é¢„æµ‹é¡¹ç›®ä¸»å…¥å£
æ”¯æŒæ‰¹é‡è®­ç»ƒã€æ¨¡å‹æ¯”è¾ƒã€å¯æ§æµç¨‹
"""
import os
import sys
import argparse
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.config.config_manager import ConfigManager
from src.data.loader import DataLoader
from src.data.preprocessor import DataPreprocessor
from src.data.dataset import create_datasets
from src.models.deep_learning import create_deep_learning_model
from src.models.traditional_ml import create_traditional_ml_model
from src.evaluation.evaluator import ModelEvaluator
from src.training.train_dl import train_deep_learning_model

import torch
import torch.nn as nn
from torch.utils.data import DataLoader as TorchDataLoader
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional
import json
import warnings
warnings.filterwarnings('ignore')


class ExperimentRunner:
    """å®éªŒè¿è¡Œå™¨ - ç»Ÿä¸€ç®¡ç†æ•´ä¸ªå®éªŒæµç¨‹"""
    
    def __init__(self, config_path: str = "config/config.yaml"):
        """
        åˆå§‹åŒ–å®éªŒè¿è¡Œå™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = ConfigManager(config_path)
        self.data_loader = None
        self.preprocessor = None
        self.datasets = {}
        self.evaluator = ModelEvaluator(str(self.config.get_output_path('plots')))
        self.results = {}
        
        # è®¾ç½®éšæœºç§å­
        self._set_random_seed()
        
        print("=== ADHDè®¤çŸ¥éšœç¢é¢„æµ‹å®éªŒç³»ç»Ÿ ===")
        print(f"å®éªŒåç§°: {self.config.get('experiment.name')}")
        
    def _set_random_seed(self):
        """è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§"""
        seed = self.config.get('random_seed', 42)
        
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
        
        print(f"éšæœºç§å­è®¾ç½®ä¸º: {seed}")
    
    def run_experiment(self, 
                      steps: Optional[List[str]] = None,
                      tasks: Optional[List[str]] = None,
                      models: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´å®éªŒ
        
        Args:
            steps: è¦æ‰§è¡Œçš„æ­¥éª¤åˆ—è¡¨ ['data', 'training', 'evaluation']
            tasks: è¦è¿è¡Œçš„ä»»åŠ¡åˆ—è¡¨
            models: è¦è¿è¡Œçš„æ¨¡å‹åˆ—è¡¨
            
        Returns:
            å®éªŒç»“æœæ‘˜è¦
        """
        start_time = time.time()
        
        if steps is None:
            steps = ['data', 'training', 'evaluation']
        
        if tasks is None:
            tasks = self.config.get_enabled_tasks()
        
        print(f"\næ‰§è¡Œæ­¥éª¤: {steps}")
        print(f"ä»»åŠ¡: {tasks}")
        print(f"æ€»æ¨¡å‹æ•°: {len(self.config.get_enabled_models())}")
        
        try:
            # æ­¥éª¤1: æ•°æ®å‡†å¤‡
            if 'data' in steps:
                self.prepare_data()
            
            # æ­¥éª¤2: æ¨¡å‹è®­ç»ƒ
            if 'training' in steps:
                self.train_all_models(tasks, models)
            
            # æ­¥éª¤3: ç»“æœåˆ†æ
            if 'evaluation' in steps:
                self.analyze_results(tasks)
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            summary = self.generate_final_report()
            
            total_time = time.time() - start_time
            print(f"\n=== å®éªŒå®Œæˆ ===")
            print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
            
            return summary
            
        except Exception as e:
            print(f"\nâŒ å®éªŒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {"status": "failed", "error": str(e)}
    
    def prepare_data(self):
        """æ•°æ®å‡†å¤‡æ­¥éª¤"""
        print("\n=== æ­¥éª¤1: æ•°æ®å‡†å¤‡ ===")
        
        # 1. åŠ è½½åŸå§‹æ•°æ®
        print("1.1 åŠ è½½æ•°æ®...")
        data_config = self.config.get_data_config()
        self.data_loader = DataLoader(data_config.get('data_dir', 'data/cleaned'))
        load_info = self.data_loader.load_latest_data()
        
        # 2. æ•°æ®é¢„å¤„ç†
        print("1.2 æ•°æ®é¢„å¤„ç†...")
        preprocess_config = self.config.get_preprocessing_config()
        
        self.preprocessor = DataPreprocessor()
        # é…ç½®é¢„å¤„ç†å‚æ•°
        imputation_enabled = preprocess_config.get('multiple_imputation', {}).get('enabled', True)
        n_imputations = preprocess_config.get('multiple_imputation', {}).get('n_imputations', 5)
        outlier_method = preprocess_config.get('outlier_detection', {}).get('method', 'iqr')
        scaling_method = preprocess_config.get('scaling', {}).get('method', 'standard')
        
        # ä½¿ç”¨ä¸€ä¸ªä»»åŠ¡çš„æ•°æ®æ¥æ‹Ÿåˆé¢„å¤„ç†å™¨
        available_tasks = self.config.get_enabled_tasks()
        if available_tasks:
            sample_task = available_tasks[0]
            features, _ = self.data_loader.get_aligned_data(sample_task)
            
            self.preprocessor.fit(
                features,
                imputation_iterations=n_imputations if imputation_enabled else 0,
                outlier_method=outlier_method,
                scaling_method=scaling_method
            )
        
        # ä¿å­˜æ•°æ®å‡†å¤‡æŠ¥å‘Š
        data_report = {
            "load_info": load_info,
            "preprocessing_report": self.preprocessor.get_preprocessing_report()
        }
        
        report_path = self.config.get_output_path('logs', 'data_preparation_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(data_report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"æ•°æ®å‡†å¤‡å®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜è‡³: {report_path}")
    
    def train_all_models(self, tasks: List[str], model_filter: Optional[List[str]] = None):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        print("\n=== æ­¥éª¤2: æ¨¡å‹è®­ç»ƒ ===")
        
        enabled_models = self.config.get_enabled_models()
        
        if model_filter:
            enabled_models = {k: v for k, v in enabled_models.items() if k in model_filter}
        
        print(f"å°†è®­ç»ƒ {len(enabled_models)} ä¸ªæ¨¡å‹")
        
        for task_type in tasks:
            print(f"\n--- è®­ç»ƒ {task_type} ä»»åŠ¡çš„æ¨¡å‹ ---")
            
            if task_type not in self.results:
                self.results[task_type] = {}
            
            # åˆ›å»ºæ•°æ®é›†ï¼ˆæ¯ä¸ªä»»åŠ¡å•ç‹¬åˆ›å»ºï¼‰
            task_datasets = create_datasets(
                data_loader=self.data_loader,
                preprocessor=self.preprocessor,
                task_type=task_type,
                test_size=self.config.get('data.test_ratio', 0.1),
                val_size=self.config.get('data.val_ratio', 0.1),
                random_state=self.config.get('random_seed', 42)
            )
            
            for model_id, model_info in enabled_models.items():
                print(f"\nè®­ç»ƒæ¨¡å‹: {model_id}")
                
                try:
                    result = self._train_single_model(
                        model_id, model_info, task_type, task_datasets
                    )
                    self.results[task_type][model_id] = result
                    print(f"âœ… {model_id} è®­ç»ƒå®Œæˆ")
                    
                except Exception as e:
                    print(f"âŒ {model_id} è®­ç»ƒå¤±è´¥: {e}")
                    self.results[task_type][model_id] = {
                        "status": "failed",
                        "error": str(e)
                    }
    
    def _train_single_model(self, model_id: str, model_info: Dict[str, Any], 
                           task_type: str, datasets: Dict) -> Dict[str, Any]:
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        start_time = time.time()
        model_category = model_info['category']
        model_name = model_info['name']
        
        # è·å–æ•°æ®é›†ä¿¡æ¯
        train_dataset = datasets['train']
        val_dataset = datasets['val'] 
        test_dataset = datasets['test']
        
        num_features = train_dataset.get_feature_dim()
        if task_type == "classification":
            output_dim = train_dataset.get_num_classes()
        else:
            output_dim = 1
        
        # å‡†å¤‡æ¨¡å‹é…ç½®
        model_config = model_info['config']['params'].copy()
        model_config.update({
            'input_dim': num_features,
            'output_dim': output_dim,
            'task_type': task_type
        })
        
        if model_category == 'deep_learning':
            return self._train_deep_learning_model(
                model_name, model_config, datasets, model_id, task_type, start_time
            )
        else:  # traditional_ml
            return self._train_traditional_ml_model(
                model_name, model_config, datasets, model_id, task_type, start_time
            )
    
    def _train_deep_learning_model(self, model_name: str, model_config: Dict[str, Any],
                                 datasets: Dict, model_id: str, task_type: str,
                                 start_time: float) -> Dict[str, Any]:
        """è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹"""
        # åˆ›å»ºæ¨¡å‹
        model = create_deep_learning_model(model_name, model_config)
        
        # è®­ç»ƒé…ç½®
        train_config = self.config.get_training_config('deep_learning')
        device = torch.device(train_config.get('device', 'cpu'))
        model.to(device)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        batch_size = train_config.get('batch_size', 64)
        train_loader = TorchDataLoader(datasets['train'], batch_size=batch_size, shuffle=True)
        val_loader = TorchDataLoader(datasets['val'], batch_size=batch_size, shuffle=False)
        test_loader = TorchDataLoader(datasets['test'], batch_size=batch_size, shuffle=False)
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        if task_type == "classification":
            criterion = nn.CrossEntropyLoss()
        else:
            criterion = nn.MSELoss()
        
        optimizer = torch.optim.Adam(
            model.parameters(), 
            lr=train_config.get('learning_rate', 0.001)
        )
        
        # è®­ç»ƒæ¨¡å‹
        history = train_deep_learning_model(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            criterion=criterion,
            optimizer=optimizer,
            device=device,
            num_epochs=train_config.get('num_epochs', 100),
            patience=train_config.get('early_stopping', {}).get('patience', 15)
        )
        
        # ä¿å­˜æ¨¡å‹
        model_path = self.config.get_output_path('models', f"{model_id}_{task_type}.pth")
        torch.save(model.state_dict(), model_path)
        
        # è¯„ä¼°æ¨¡å‹
        eval_results = self.evaluator.evaluate_deep_learning_model(
            model=model,
            test_loader=test_loader,
            criterion=criterion,
            device=device,
            task_type=task_type
        )
        
        training_time = time.time() - start_time
        
        return {
            "status": "completed",
            "model_type": "deep_learning",
            "model_name": model_name,
            "training_history": history,
            "evaluation_results": eval_results,
            "training_time": training_time,
            "model_path": str(model_path)
        }
    
    def _train_traditional_ml_model(self, model_name: str, model_config: Dict[str, Any],
                                   datasets: Dict, model_id: str, task_type: str,
                                   start_time: float) -> Dict[str, Any]:
        """è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹"""
        # åˆ›å»ºæ¨¡å‹
        model = create_traditional_ml_model(model_name, {
            'task_type': task_type,
            'hyperparameters': model_config
        })
        
        # å‡†å¤‡æ•°æ®
        train_dataset = datasets['train']
        val_dataset = datasets['val']
        test_dataset = datasets['test']
        
        X_train, y_train = train_dataset.X, train_dataset.y
        X_val, y_val = val_dataset.X, val_dataset.y
        X_test, y_test = test_dataset.X, test_dataset.y
        
        # è®­ç»ƒæ¨¡å‹
        train_result = model.fit(X_train, y_train, X_val, y_val)
        
        # ä¿å­˜æ¨¡å‹
        model_path = self.config.get_output_path('models', f"{model_id}_{task_type}.joblib")
        model.save_model(str(model_path))
        
        # è¯„ä¼°æ¨¡å‹
        eval_results = self.evaluator.evaluate_traditional_ml_model(
            model=model,
            X_test=X_test,
            y_test=y_test,
            task_type=task_type
        )
        
        training_time = time.time() - start_time
        
        return {
            "status": "completed",
            "model_type": "traditional_ml",
            "model_name": model_name,
            "training_result": train_result,
            "evaluation_results": eval_results,
            "training_time": training_time,
            "model_path": str(model_path)
        }
    
    def analyze_results(self, tasks: List[str]):
        """åˆ†æå®éªŒç»“æœ"""
        print("\n=== æ­¥éª¤3: ç»“æœåˆ†æ ===")
        
        for task_type in tasks:
            if task_type not in self.results:
                continue
                
            print(f"\n--- åˆ†æ {task_type} ç»“æœ ---")
            
            # æ”¶é›†æˆåŠŸçš„ç»“æœ
            successful_results = {
                model_id: result for model_id, result in self.results[task_type].items()
                if result.get("status") == "completed"
            }
            
            if not successful_results:
                print(f"âŒ {task_type} ä»»åŠ¡æ²¡æœ‰æˆåŠŸçš„æ¨¡å‹")
                continue
            
            # åˆ›å»ºæ¯”è¾ƒè¡¨æ ¼
            comparison_data = []
            for model_id, result in successful_results.items():
                eval_results = result.get("evaluation_results", {})
                row = {
                    "model_id": model_id,
                    "model_type": result.get("model_type"),
                    "model_name": result.get("model_name"),
                    "training_time": result.get("training_time", 0)
                }
                
                # æ·»åŠ è¯„ä¼°æŒ‡æ ‡
                if task_type == "classification":
                    metrics = ["accuracy", "precision", "recall", "f1_score", "roc_auc"]
                else:
                    metrics = ["mse", "rmse", "mae", "r2_score"]
                
                for metric in metrics:
                    row[metric] = eval_results.get(metric, None)
                
                comparison_data.append(row)
            
            # åˆ›å»ºæ¯”è¾ƒDataFrame
            comparison_df = pd.DataFrame(comparison_data)
            
            # ä¿å­˜æ¯”è¾ƒç»“æœ
            comparison_path = self.config.get_output_path('results', f"{task_type}_model_comparison.csv")
            comparison_df.to_csv(comparison_path, index=False)
            
            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
            if task_type == "classification":
                best_idx = comparison_df["f1_score"].idxmax()
                primary_metric = "f1_score"
            else:
                best_idx = comparison_df["r2_score"].idxmax()
                primary_metric = "r2_score"
            
            best_model = comparison_df.loc[best_idx, "model_id"]
            best_score = comparison_df.loc[best_idx, primary_metric]
            
            print(f"âœ… æœ€ä½³ {task_type} æ¨¡å‹: {best_model} ({primary_metric}={best_score:.4f})")
            print(f"æ¯”è¾ƒç»“æœä¿å­˜è‡³: {comparison_path}")
    
    def generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆå®éªŒæŠ¥å‘Š"""
        print("\n=== ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š ===")
        
        # ç»Ÿè®¡ç»“æœ
        total_models = 0
        successful_models = 0
        failed_models = 0
        
        task_summaries = {}
        
        for task_type, task_results in self.results.items():
            total_task_models = len(task_results)
            successful_task_models = len([r for r in task_results.values() if r.get("status") == "completed"])
            failed_task_models = total_task_models - successful_task_models
            
            task_summaries[task_type] = {
                "total_models": total_task_models,
                "successful_models": successful_task_models,
                "failed_models": failed_task_models
            }
            
            total_models += total_task_models
            successful_models += successful_task_models
            failed_models += failed_task_models
        
        # ç”ŸæˆæŠ¥å‘Š
        final_report = {
            "experiment_info": {
                "name": self.config.get('experiment.name'),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "config_summary": self.config.get_summary()
            },
            "overall_statistics": {
                "total_models_trained": total_models,
                "successful_models": successful_models,
                "failed_models": failed_models,
                "success_rate": successful_models / total_models if total_models > 0 else 0
            },
            "task_summaries": task_summaries,
            "output_directory": str(self.config.output_dirs['base'])
        }
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        report_path = self.config.get_output_path('results', 'final_experiment_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        print(f"æˆåŠŸè®­ç»ƒæ¨¡å‹: {successful_models}/{total_models}")
        
        return final_report


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ADHDè®¤çŸ¥éšœç¢é¢„æµ‹å®éªŒç³»ç»Ÿ')
    parser.add_argument('--config', '-c', default='config/config.yaml', 
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--steps', nargs='+', 
                       choices=['data', 'training', 'evaluation'],
                       default=['data', 'training', 'evaluation'],
                       help='è¦æ‰§è¡Œçš„æ­¥éª¤')
    parser.add_argument('--tasks', nargs='+',
                       choices=['classification', 'regression'],
                       help='è¦è¿è¡Œçš„ä»»åŠ¡ï¼ˆé»˜è®¤è¿è¡Œé…ç½®ä¸­å¯ç”¨çš„æ‰€æœ‰ä»»åŠ¡ï¼‰')
    parser.add_argument('--models', nargs='+',
                       help='è¦è®­ç»ƒçš„æ¨¡å‹IDï¼ˆé»˜è®¤è®­ç»ƒæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not Path(args.config).exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        return 1
    
    try:
        # åˆ›å»ºå®éªŒè¿è¡Œå™¨
        runner = ExperimentRunner(args.config)
        
        # è¿è¡Œå®éªŒ
        summary = runner.run_experiment(
            steps=args.steps,
            tasks=args.tasks,
            models=args.models
        )
        
        if summary.get('status') == 'failed':
            return 1
            
        print("\nğŸ‰ å®éªŒæˆåŠŸå®Œæˆï¼")
        return 0
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­å®éªŒ")
        return 130
    except Exception as e:
        print(f"\nğŸ’¥ å®éªŒå¼‚å¸¸ç»ˆæ­¢: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())